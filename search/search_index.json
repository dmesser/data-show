{
    "docs": [
        {
            "location": "/", 
            "text": "Getting Started\n#\n\n\n\n\nLab Tips\n\n\n\n\nTo start the Test Drive press the \nSTART\n button from the Lab Home Screen\n\n\nOnce your lab environment is provisioned, gather the login information from the \nConnection Details\n section\n\n\nYour SSH Login Credentials are : User Name: \nstudent\n Password: \nRedhat18\n\n\nTo open this lab guide in a separate browser window use \nThis Link\n.\n\n\nCompleting the entire lab content will typically take between 2:30 to 3:00 hours (your mileage may vary)\n\n\nGenerously use copy-to-clipboard button next to each command.\n\n\nPlease send your feedback/suggestions or bug reports to \n\n\nThis is a temporary environment for training and learning purposes. Do not host production or test workloads.\n\n\n\n\n\n\nIntroduction\n#\n\n\nWelcome to the Red Hat Ceph Storage Test Drive. \n\n\nFor a smooth learning experience, the contents of this test drive have been divided into the following modules. Each of these modules are independent of each other. You can continue the lab in sequence, or, if you would rather pick and choose which modules to work on, and in which ever order you desire, simply fulfill the prerequisite build which is included with each module. \n\n\n\n\nModule 1 :\n Introduction to Red Hat Ceph Storage\n\n\nModule 2 :\n Deploying Red Hat Ceph Storage\n\n\nModule 3 :\n Ceph Block Storage Provisioning\n\n\nModule 4 :\n Ceph Object Storage Provisioning\n\n\nModule 5 :\n Ceph File Storage Provisioning\n\n\nModule 6 :\n Ceph Administration\n\n\n\n\n\n\n\n\nTest Drive Prerequisites\n#\n\n\nFor this lab you need the following:\n\n\n\n\nWorkstation with Internet access\n\n\nSSH client program installed on your workstation\n\n\n\n\nStarting the LAB\n#\n\n\nAfter you have logged into your Red Hat Test Drive Portal, select \nRed Hat Ceph Stoage 3\n Test Drive.\n\n\n\n\nTo start the Test Drive press \nSTART\n button in the top bar.\n\n\nGenerally it takes less than 10 minutes to provision your lab environment.\n\n\nIn total, you can start the lab 5 times. After that, you will reach your quota of free labs. Contact us if you want to run more labs.\n\n\n\n\nAccessing the LAB\n#\n\n\nOnce your lab environment is provisioned, you will find login details on the left hand pane in a section labeled \nConnection Details\n \n\n\n\n\nSSH IP Address\n\n\nSSH user name\n\n\nSSH Password\n\n\n\n\nHaving all these details, open your terminal or SSH program and run \n\n\nssh student@\nSSH IP Address\n\n\n\n\n\nTerminating the LAB\n#\n\n\n\n\nYour lab will auto terminate in 180 minutes, however, if you like, you can click the \nEnd Lab\n button to terminate the lab.\n\n\nTell us about your lab experience and suggestions for improvements in the feedback box.\n\n\n\n\nLab Environment Overview\n#\n\n\n\n\nThe lab environment has 6 nodes in total and you will be using \nceph-admin\n node most of the time. \n\n\nAll the nodes have Internet access, however only the \nceph-admin\n node will be reachable via public IP.\n\n\n\n\n\n\n\n\n\n\nNode Name\n\n\nFunction\n\n\n\n\n\n\n\n\n\n\nceph-admin\n\n\nCeph (Ansible + Metrics + RGW + Client)\n\n\n\n\n\n\nceph-node1\n\n\nCeph (MON + Filestore OSD)\n\n\n\n\n\n\nceph-node2\n\n\nCeph (MON + Filestore OSD)\n\n\n\n\n\n\nceph-node3\n\n\nCeph (MON + Filestore OSD)\n\n\n\n\n\n\nceph-node4\n\n\nCeph (Bluestore OSD)\n\n\n\n\n\n\nclient-node1\n\n\nCeph Client\n\n\n\n\n\n\n\n\nRed Hat Ceph Storage Prerequisites\n#\n\n\nSetting up Red Hat Ceph Storage (RHCS) requires configuration of the cluster nodes.\n\n\n\n\nOperating System :\n  Supported version of OS.\n\n\nRegistration to RHN :\n To get OS/RHCS packages for installation.\n\n\nSeparate networks :\n For Ceph Public and Cluster traffic.\n\n\nSetting hostname resolutions :\n Either local or DNS name resolution \n\n\nConfiguring firewall :\n  Allow necessary port to be opened.\n\n\nNTP configuration:\n For time synchronization across nodes.\n\n\nLocal user account:\n User with password less sudo ssh access to all nodes.\n\n\n\n\n\n\nYou are covered\n\n\nThe purpose of the Red Hat Ceph Storage Test Drive is to provide you with an environment where you can focus on learning this great technology, without spending time fulfilling prerequisites. All the prerequisites listed above have been taken care of for this course.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Lab Tips   To start the Test Drive press the  START  button from the Lab Home Screen  Once your lab environment is provisioned, gather the login information from the  Connection Details  section  Your SSH Login Credentials are : User Name:  student  Password:  Redhat18  To open this lab guide in a separate browser window use  This Link .  Completing the entire lab content will typically take between 2:30 to 3:00 hours (your mileage may vary)  Generously use copy-to-clipboard button next to each command.  Please send your feedback/suggestions or bug reports to   This is a temporary environment for training and learning purposes. Do not host production or test workloads.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to the Red Hat Ceph Storage Test Drive.   For a smooth learning experience, the contents of this test drive have been divided into the following modules. Each of these modules are independent of each other. You can continue the lab in sequence, or, if you would rather pick and choose which modules to work on, and in which ever order you desire, simply fulfill the prerequisite build which is included with each module.    Module 1 :  Introduction to Red Hat Ceph Storage  Module 2 :  Deploying Red Hat Ceph Storage  Module 3 :  Ceph Block Storage Provisioning  Module 4 :  Ceph Object Storage Provisioning  Module 5 :  Ceph File Storage Provisioning  Module 6 :  Ceph Administration", 
            "title": "Introduction"
        }, 
        {
            "location": "/#test-drive-prerequisites", 
            "text": "For this lab you need the following:   Workstation with Internet access  SSH client program installed on your workstation", 
            "title": "Test Drive Prerequisites"
        }, 
        {
            "location": "/#starting-the-lab", 
            "text": "After you have logged into your Red Hat Test Drive Portal, select  Red Hat Ceph Stoage 3  Test Drive.   To start the Test Drive press  START  button in the top bar.  Generally it takes less than 10 minutes to provision your lab environment.  In total, you can start the lab 5 times. After that, you will reach your quota of free labs. Contact us if you want to run more labs.", 
            "title": "Starting the LAB"
        }, 
        {
            "location": "/#accessing-the-lab", 
            "text": "Once your lab environment is provisioned, you will find login details on the left hand pane in a section labeled  Connection Details     SSH IP Address  SSH user name  SSH Password   Having all these details, open your terminal or SSH program and run   ssh student@ SSH IP Address", 
            "title": "Accessing the LAB"
        }, 
        {
            "location": "/#terminating-the-lab", 
            "text": "Your lab will auto terminate in 180 minutes, however, if you like, you can click the  End Lab  button to terminate the lab.  Tell us about your lab experience and suggestions for improvements in the feedback box.", 
            "title": "Terminating the LAB"
        }, 
        {
            "location": "/#lab-environment-overview", 
            "text": "The lab environment has 6 nodes in total and you will be using  ceph-admin  node most of the time.   All the nodes have Internet access, however only the  ceph-admin  node will be reachable via public IP.      Node Name  Function      ceph-admin  Ceph (Ansible + Metrics + RGW + Client)    ceph-node1  Ceph (MON + Filestore OSD)    ceph-node2  Ceph (MON + Filestore OSD)    ceph-node3  Ceph (MON + Filestore OSD)    ceph-node4  Ceph (Bluestore OSD)    client-node1  Ceph Client", 
            "title": "Lab Environment Overview"
        }, 
        {
            "location": "/#red-hat-ceph-storage-prerequisites", 
            "text": "Setting up Red Hat Ceph Storage (RHCS) requires configuration of the cluster nodes.   Operating System :   Supported version of OS.  Registration to RHN :  To get OS/RHCS packages for installation.  Separate networks :  For Ceph Public and Cluster traffic.  Setting hostname resolutions :  Either local or DNS name resolution   Configuring firewall :   Allow necessary port to be opened.  NTP configuration:  For time synchronization across nodes.  Local user account:  User with password less sudo ssh access to all nodes.    You are covered  The purpose of the Red Hat Ceph Storage Test Drive is to provide you with an environment where you can focus on learning this great technology, without spending time fulfilling prerequisites. All the prerequisites listed above have been taken care of for this course.", 
            "title": "Red Hat Ceph Storage Prerequisites"
        }, 
        {
            "location": "/Module-1/", 
            "text": "Module - 1 : Introduction to Red Hat Ceph Storage\n#\n\n\n\n\nModule Overview\n\n\nModule 1 provides an introduction to Software Defined Storage, the Red Hat Ceph architecture, target use cases, and features. There is no hands-on exercise to be performed in this module.\n\n\n\n\nIf you are familiar with Red Hat Ceph Storage, peruse Module 1 as a refresher, or you may skip it altogether. Once you are ready for the action, please proceed to \nModule 2\n\n\n\n\nSoftware Defined Storage and Ceph\n#\n\n\n\n\n\n\n\n\n\n\n\n\nWhat\ns New in Ceph 3\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCeph Architectural Components\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCeph Use Cases\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCeph Features\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere to get more information\n#\n\n\n\n\n\n\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-1. At this point you should have theoretical understanding of software defined storage concepts, Ceph\ns architecture and its terminology. In the next module you will learn how to deploy a Ceph cluster.", 
            "title": "Module-1 : Ceph cluster & Object Storage Setup"
        }, 
        {
            "location": "/Module-1/#module-1-introduction-to-red-hat-ceph-storage", 
            "text": "Module Overview  Module 1 provides an introduction to Software Defined Storage, the Red Hat Ceph architecture, target use cases, and features. There is no hands-on exercise to be performed in this module.   If you are familiar with Red Hat Ceph Storage, peruse Module 1 as a refresher, or you may skip it altogether. Once you are ready for the action, please proceed to  Module 2", 
            "title": "Module - 1 : Introduction to Red Hat Ceph Storage"
        }, 
        {
            "location": "/Module-1/#software-defined-storage-and-ceph", 
            "text": "", 
            "title": "Software Defined Storage and Ceph"
        }, 
        {
            "location": "/Module-1/#whats-new-in-ceph-3", 
            "text": "", 
            "title": "What's New in Ceph 3"
        }, 
        {
            "location": "/Module-1/#ceph-architectural-components", 
            "text": "", 
            "title": "Ceph Architectural Components"
        }, 
        {
            "location": "/Module-1/#ceph-use-cases", 
            "text": "", 
            "title": "Ceph Use Cases"
        }, 
        {
            "location": "/Module-1/#ceph-features", 
            "text": "", 
            "title": "Ceph Features"
        }, 
        {
            "location": "/Module-1/#where-to-get-more-information", 
            "text": "End of Module  We have reached the end of Module-1. At this point you should have theoretical understanding of software defined storage concepts, Ceph s architecture and its terminology. In the next module you will learn how to deploy a Ceph cluster.", 
            "title": "Where to get more information"
        }, 
        {
            "location": "/Module-2/", 
            "text": "Module - 2 : Deploying Red Hat Ceph Storage\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module you will be deploying Red Hat Ceph Storage 3 cluster across 3 nodes using Ansible based deployer called \nceph-ansible\n.\n\n\nLater you will deploy Ceph Metrics, an end to end monitoring system for Red Hat Ceph Storage.\n\n\nBy the end of this module you will have a running Red Hat Ceph Storage cluster and Ceph Metrics monitoring Dashboard.\n\n\n\n\n\n\n\n\nFrom your workstation login to the \nceph-admin\n node with the user name \nstudent\n \n(Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-admin node\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou must run all the commands logged in as user \nstudent\n on the \nceph-admin\n node, unless otherwise specified. \n\n\n\n\n\n\nSetting up environment for ceph-ansible\n#\n\n\nIn-order to save time \nceph-ansible\n package has already been installed on \nceph-admin\n node.\n\n\n\n\nBegin by creating a directory for ceph-ansible keys under \nstudent\n user\ns home directory.\n\n\n\n\nmkdir ~/ceph-ansible-keys\n\n\n\n\n\n\nCreate a new ansible inventory file which helps \nceph-ansible\n to know what role needs to be applied on each node.\n\n\n\n\nsudo vi /etc/ansible/hosts\n\n\n\n\n\n\nIn the \n/etc/ansible/hosts\n inventory file add \n\n\nCeph Monitor host name \nceph-node[1:3]\n under \n[mons]\n  section\n\n\nCeph OSDs host name \nceph-node[1:3]\n under \n[osds]\n  section\n\n\nCeph Manager host name \nceph-admin\n under \n[mgrs]\n  section\n\n\nCeph Client host name \nceph-admin\n under \n[clients]\n  section \n\n\n\n\n\n\n\n\n[mons]\n\n\nceph-node[1:3]\n\n\n\n[osds]\n\n\nceph-node[1:3]\n\n\n\n[mgrs]\n\n\nceph-admin\n\n\n\n[clients]\n\n\nceph-admin\n\n\n\n\n\n\n\nInfo\n\n\n\n\nSince this is a lab environment we are collocating Ceph Mon and Ceph OSD daemons on \nceph-node*\n nodes\n\n\nAlso \nceph-admin\n node will host Ceph Manager and Ceph Client services\n\n\n\n\n\n\n\n\nEnsure that Ansible can reach to all the cluster nodes.\n\n\n\n\nansible all -m ping\n\n\n\n\nConfiguring Ceph-Ansible Settings\n#\n\n\n\n\nVisit \nceph-ansible\n main configuration directory\n\n\n\n\ncd /usr/share/ceph-ansible/group_vars/\n\n\n\n\n\n\nIn the directory you will find \nall.yml\n , \nosds.yml\n and \nclients.yml\n configuration files which are \npre-populated for you\n to avoid any typographic errors. Lets look at these configuration files one by one.\n\n\n\n\n\n\nTip\n\n\nYou can skip editing configuration files as they are pre-populated with correct settings to avoid typos and save time.\n\n\n\n\n\n\nall.yml\n configuration file most importantly configures\n\n\nCeph repository, path to RHCS ISO\n\n\nCeph Monitor network interface ID, public network\n\n\nCeph OSD backend as \nfilestore\n ( In later modules you will test \nbluestore\n too )\n\n\nCeph RGW port, threads and interface (Required for module - 4)\n\n\nCeph configuration settings for pools\n\n\n\n\n\n\n\n\ncat all.yml\n\n\n\n\n---\n\n\ndummy:\n\n\nfetch_directory: ~/ceph-ansible-keys\n\n\nceph_repository_type: iso\n\n\nceph_origin: repository\n\n\nceph_repository: rhcs\n\n\nceph_rhcs_version: 3\n\n\nceph_rhcs_iso_path: \n/home/student/rhceph-3.0-rhel-7-x86_64.iso\n\n\n\nmonitor_interface: eth0\n\n\nmon_use_fqdn: true\n\n\npublic_network: 10.100.0.0/16\n\n\nosd_objectstore: filestore\n\n\n\nradosgw_civetweb_port: 80\n\n\nradosgw_civetweb_num_threads: 512\n\n\nradosgw_civetweb_options: \nnum_threads=\n{{\n \nradosgw_civetweb_num_threads\n \n}}\n\n\nradosgw_interface: eth0\n\n\nradosgw_dns_name: \nceph-admin\n\n\n\nceph_conf_overrides:\n\n\n  global:\n\n\n    osd pool default pg num: 64\n\n\n    osd pool default pgp num: 64\n\n\n    mon allow pool delete: true\n\n\n    mon clock drift allowed: 5\n\n\n\n\n\n\n\nosds.yml\n configuration file most importantly configures\n\n\nCeph OSD deployment scenario to be collocated (ceph-data and ceph-journal on same device)\n\n\nAuto discover storage device and use them as Ceph OSD\n\n\nAllow Ceph OSD nodes to be ceph admin nodes (optional)\n\n\n\n\n\n\n\n\ncat osds.yml\n\n\n\n\n---\ndummy:\ncopy_admin_key: true\nosd_auto_discovery: true\nosd_scenario: collocated\n\n\n\n\n\n\nclients.yml\n configuration file most importantly configures\n\n\nAllow Ceph client nodes to issue ceph admin commands (optional, not recomended for production)\n\n\n\n\n\n\n\n\ncat clients.yml\n\n\n\n\n---\ndummy:\ncopy_admin_key: True\n\n\n\n\nDeploying RHCS Cluster\n#\n\n\n\n\nTo start deploying RHCS cluster, switch to \nceph-ansible\n root directory\n\n\n\n\ncd /usr/share/ceph-ansible\n\n\n\n\n\n\nRun \nceph-ansible\n playbook\n\n\n\n\ntime ansible-playbook site.yml\n\n\n\n\n\n\nThis should usually take 10-12 minutes to complete. Once its done, play recap should look similar to below. Make sure play recap does not show any host run failed.\n\n\n\n\nPLAY RECAP ******************************************************************\nceph-node1                 : ok=149  changed=29   unreachable=0    failed=0\nceph-node2                 : ok=136  changed=24   unreachable=0    failed=0\nceph-node3                 : ok=138  changed=25   unreachable=0    failed=0\n\nreal  10m9.966s\nuser  2m6.029s\nsys 1m1.005s\n\n\n\n\n\n\nFinally check the status of your cluster. \n\n\n\n\nsudo ceph -s\n\n\n\n\n[student@ceph-admin ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-admin(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-admin ceph-ansible]$\n\n\n\n\n\n\nSuccess\n\n\nAt this point you should have a healthy RHCS cluster up and running with 3 x Ceph Monitors, 3 x Ceph OSDs (12 x OSDs), 1 x Ceph Manager.\n\n\n\n\nDeploying Ceph Metrics Dashboard\n#\n\n\nRHCS 3 comes with a brand new Ceph monitoring interface, known as Ceph Metrics Dashboard. In this section, we shall deploy Ceph Metrics Dashboard which comes with its own installer called \ncephmetrics-ansible\n\n\n\n\nEdit Ansible inventory file\n\n\n\n\nsudo vim /etc/ansible/hosts\n\n\n\n\n\n\nAdd the name of the host you like use for Ceph Metrics, in our case we will use \nceph-admin\n node. Add the following content to \n/etc/ansible/hosts\n file.\n\n\n\n\n[ceph-grafana]\n\n\nceph-admin ansible_connection\n=\nlocal\n\n\n\n\n\n\n\nSwitch to Ceph Metrics Ansible directory\n\n\n\n\ncd /usr/share/cephmetrics-ansible\n\n\n\n\n\n\nDeploy Ceph Metrics \n\n\n\n\ntime ansible-playbook playbook.yml\n\n\n\n\n\n\n\n\nThis should usually take under 5 minutes to complete. Once its done, make sure play recap does not show any host run failed.\n\n\n\n\n\n\nTo open Ceph Metrics Dashboard, grab your \nPublic IP Address of ceph-admin node (the IP you ssh into)\n and point your workstation web browser to \n\n\n\n\n\n\nhttp://\nceph-admin_Public-IP_Address\n:3000\n\n\n\n\n\n\nUser Name : \nadmin\n\n\n\n\nPassword : \nadmin\n\n\n\n\n\n\nThe Ceph Metrics Dashboard should look like this.\n\n\n\n\n\n\n\n\n(click on the screen shot for better resolution)\n\n\n\n\nThere are a lot of stats that you can monitor using Ceph Metrics, just select a dashboard from the drop-down list as shown in the screen shot. \n\n\n\n\n\n\n\n\nTip\n\n\nDon\nt worry if some of the panels in Ceph Metrics Dashboard is not showing any data or showing NA. Its simply because its a new cluster and does not have any data on it.\n\n\n\n\nInteracting with your Ceph cluster\n#\n\n\nIn this section you will learn a few commands to interact with Ceph cluster. These commands should be executed from \nceph-admin\n node as \nstudent\n user.\n\n\n\n\nAllow \nstudent\n user to access ceph cluster\n\n\n\n\nsudo chown -R student:student /etc/ceph\n\n\n\n\n\n\nCheck cluster status\n\n\n\n\nceph -s\n\n\n\n\n\n\nCheck cluster health\n\n\n\n\nceph health detail\n\n\n\n\n\n\nCheck Ceph OSD stats and tree view of OSDs in cluster\n\n\n\n\nceph osd stat\n\n\n\n\nceph osd tree\n\n\n\n\n\n\nCheck Ceph monitor status\n\n\n\n\nceph mon stat\n\n\n\n\nCreating a Ceph Storage Pool\n#\n\n\nWe now have a running Ceph cluster. In order to use it, we will need to create a storage pool. Based on the data protection schemes, a storage pool in Ceph could be one of two types \n\n- \nReplicated\n : Data is replicated for protection. \n\n- \nErasure Coded\n : Data is chunked and stored together with erasure coded chunks for protection.\n\n\n\n\nNote\n\n\nThe Red Hat Ceph Storage Erasure Coding method avoids capacity overhead associated with the Replication method. For data protection, instead of copying the data multiple times, EC splits the data in \nk\n pieces of Data Chunks and adds \nm\n pieces of EC chunks. The Erasure Coding scheme is referred to as \nk+m\n. Its worth noting that only 4+2, 8+3 and 8+4 EC configurations are supported by Red Hat.\n\n\nSince this is a test environment, we will create a pool with EC 2+1 configuration (as we have 3 OSD hosts). This configuration is not recommended and we are using it here only as an example.\n\n\n\n\n\n\nTo create a EC pool we first need to define the EC scheme by creating a EC profile.\n\n\n\n\nceph osd erasure-code-profile set my_ec_profile k=2 m=1 ruleset-failure-domain=host\n\n\n\n\n\n\nNext create a pool that uses the EC profile we just created\n\n\n\n\nceph osd pool create ecpool 128 128 erasure my_ec_profile\n\n\n\n\n\n\nVerify pool creation\n\n\n\n\nceph df\n\n\n\n\n\n\nStarting RHCS 3, a Pool must be associated with a application, so lets enable application on the newly created pool.\n\n\n\n\nceph osd pool application enable ecpool benchmarking\n\n\n\n\n\n\nLets write/read some data to/from this pool using \nrados bench\n tool, which runs a basic benchmarking on a pool.\n\n\n\n\n\n\nTip\n\n\nAs soon as you start generating test data by running following command, switch to Ceph Metrics Dashboard and monitor Ceph cluster activity.\n\n\n\n\nrados bench -p ecpool 60 write  --no-cleanup\n\n\n\n\nrados bench -p ecpool 60 rand\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached to the end of Module-2. At this point you have learned to deploy, configure and interact with your Ceph cluster together with Ceph Metrics monitoring dashboard. In the next module you will learn Ceph Block storage provisioning.", 
            "title": "Module-2 : OCP Setup"
        }, 
        {
            "location": "/Module-2/#module-2-deploying-red-hat-ceph-storage", 
            "text": "Module Agenda   In this module you will be deploying Red Hat Ceph Storage 3 cluster across 3 nodes using Ansible based deployer called  ceph-ansible .  Later you will deploy Ceph Metrics, an end to end monitoring system for Red Hat Ceph Storage.  By the end of this module you will have a running Red Hat Ceph Storage cluster and Ceph Metrics monitoring Dashboard.     From your workstation login to the  ceph-admin  node with the user name  student   (Learn how to Login)   ssh student@ IP Address of ceph-admin node    Prerequisite   You must run all the commands logged in as user  student  on the  ceph-admin  node, unless otherwise specified.", 
            "title": "Module - 2 : Deploying Red Hat Ceph Storage"
        }, 
        {
            "location": "/Module-2/#setting-up-environment-for-ceph-ansible", 
            "text": "In-order to save time  ceph-ansible  package has already been installed on  ceph-admin  node.   Begin by creating a directory for ceph-ansible keys under  student  user s home directory.   mkdir ~/ceph-ansible-keys   Create a new ansible inventory file which helps  ceph-ansible  to know what role needs to be applied on each node.   sudo vi /etc/ansible/hosts   In the  /etc/ansible/hosts  inventory file add   Ceph Monitor host name  ceph-node[1:3]  under  [mons]   section  Ceph OSDs host name  ceph-node[1:3]  under  [osds]   section  Ceph Manager host name  ceph-admin  under  [mgrs]   section  Ceph Client host name  ceph-admin  under  [clients]   section      [mons]  ceph-node[1:3]  [osds]  ceph-node[1:3]  [mgrs]  ceph-admin  [clients]  ceph-admin    Info   Since this is a lab environment we are collocating Ceph Mon and Ceph OSD daemons on  ceph-node*  nodes  Also  ceph-admin  node will host Ceph Manager and Ceph Client services     Ensure that Ansible can reach to all the cluster nodes.   ansible all -m ping", 
            "title": "Setting up environment for ceph-ansible"
        }, 
        {
            "location": "/Module-2/#configuring-ceph-ansible-settings", 
            "text": "Visit  ceph-ansible  main configuration directory   cd /usr/share/ceph-ansible/group_vars/   In the directory you will find  all.yml  ,  osds.yml  and  clients.yml  configuration files which are  pre-populated for you  to avoid any typographic errors. Lets look at these configuration files one by one.    Tip  You can skip editing configuration files as they are pre-populated with correct settings to avoid typos and save time.    all.yml  configuration file most importantly configures  Ceph repository, path to RHCS ISO  Ceph Monitor network interface ID, public network  Ceph OSD backend as  filestore  ( In later modules you will test  bluestore  too )  Ceph RGW port, threads and interface (Required for module - 4)  Ceph configuration settings for pools     cat all.yml  ---  dummy:  fetch_directory: ~/ceph-ansible-keys  ceph_repository_type: iso  ceph_origin: repository  ceph_repository: rhcs  ceph_rhcs_version: 3  ceph_rhcs_iso_path:  /home/student/rhceph-3.0-rhel-7-x86_64.iso  monitor_interface: eth0  mon_use_fqdn: true  public_network: 10.100.0.0/16  osd_objectstore: filestore  radosgw_civetweb_port: 80  radosgw_civetweb_num_threads: 512  radosgw_civetweb_options:  num_threads= {{   radosgw_civetweb_num_threads   }}  radosgw_interface: eth0  radosgw_dns_name:  ceph-admin  ceph_conf_overrides:    global:      osd pool default pg num: 64      osd pool default pgp num: 64      mon allow pool delete: true      mon clock drift allowed: 5    osds.yml  configuration file most importantly configures  Ceph OSD deployment scenario to be collocated (ceph-data and ceph-journal on same device)  Auto discover storage device and use them as Ceph OSD  Allow Ceph OSD nodes to be ceph admin nodes (optional)     cat osds.yml  ---\ndummy:\ncopy_admin_key: true\nosd_auto_discovery: true\nosd_scenario: collocated   clients.yml  configuration file most importantly configures  Allow Ceph client nodes to issue ceph admin commands (optional, not recomended for production)     cat clients.yml  ---\ndummy:\ncopy_admin_key: True", 
            "title": "Configuring Ceph-Ansible Settings"
        }, 
        {
            "location": "/Module-2/#deploying-rhcs-cluster", 
            "text": "To start deploying RHCS cluster, switch to  ceph-ansible  root directory   cd /usr/share/ceph-ansible   Run  ceph-ansible  playbook   time ansible-playbook site.yml   This should usually take 10-12 minutes to complete. Once its done, play recap should look similar to below. Make sure play recap does not show any host run failed.   PLAY RECAP ******************************************************************\nceph-node1                 : ok=149  changed=29   unreachable=0    failed=0\nceph-node2                 : ok=136  changed=24   unreachable=0    failed=0\nceph-node3                 : ok=138  changed=25   unreachable=0    failed=0\n\nreal  10m9.966s\nuser  2m6.029s\nsys 1m1.005s   Finally check the status of your cluster.    sudo ceph -s  [student@ceph-admin ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-admin(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-admin ceph-ansible]$   Success  At this point you should have a healthy RHCS cluster up and running with 3 x Ceph Monitors, 3 x Ceph OSDs (12 x OSDs), 1 x Ceph Manager.", 
            "title": "Deploying RHCS Cluster"
        }, 
        {
            "location": "/Module-2/#deploying-ceph-metrics-dashboard", 
            "text": "RHCS 3 comes with a brand new Ceph monitoring interface, known as Ceph Metrics Dashboard. In this section, we shall deploy Ceph Metrics Dashboard which comes with its own installer called  cephmetrics-ansible   Edit Ansible inventory file   sudo vim /etc/ansible/hosts   Add the name of the host you like use for Ceph Metrics, in our case we will use  ceph-admin  node. Add the following content to  /etc/ansible/hosts  file.   [ceph-grafana]  ceph-admin ansible_connection = local    Switch to Ceph Metrics Ansible directory   cd /usr/share/cephmetrics-ansible   Deploy Ceph Metrics    time ansible-playbook playbook.yml    This should usually take under 5 minutes to complete. Once its done, make sure play recap does not show any host run failed.    To open Ceph Metrics Dashboard, grab your  Public IP Address of ceph-admin node (the IP you ssh into)  and point your workstation web browser to     http:// ceph-admin_Public-IP_Address :3000   User Name :  admin   Password :  admin    The Ceph Metrics Dashboard should look like this.     (click on the screen shot for better resolution)   There are a lot of stats that you can monitor using Ceph Metrics, just select a dashboard from the drop-down list as shown in the screen shot.      Tip  Don t worry if some of the panels in Ceph Metrics Dashboard is not showing any data or showing NA. Its simply because its a new cluster and does not have any data on it.", 
            "title": "Deploying Ceph Metrics Dashboard"
        }, 
        {
            "location": "/Module-2/#interacting-with-your-ceph-cluster", 
            "text": "In this section you will learn a few commands to interact with Ceph cluster. These commands should be executed from  ceph-admin  node as  student  user.   Allow  student  user to access ceph cluster   sudo chown -R student:student /etc/ceph   Check cluster status   ceph -s   Check cluster health   ceph health detail   Check Ceph OSD stats and tree view of OSDs in cluster   ceph osd stat  ceph osd tree   Check Ceph monitor status   ceph mon stat", 
            "title": "Interacting with your Ceph cluster"
        }, 
        {
            "location": "/Module-2/#creating-a-ceph-storage-pool", 
            "text": "We now have a running Ceph cluster. In order to use it, we will need to create a storage pool. Based on the data protection schemes, a storage pool in Ceph could be one of two types  \n-  Replicated  : Data is replicated for protection.  \n-  Erasure Coded  : Data is chunked and stored together with erasure coded chunks for protection.   Note  The Red Hat Ceph Storage Erasure Coding method avoids capacity overhead associated with the Replication method. For data protection, instead of copying the data multiple times, EC splits the data in  k  pieces of Data Chunks and adds  m  pieces of EC chunks. The Erasure Coding scheme is referred to as  k+m . Its worth noting that only 4+2, 8+3 and 8+4 EC configurations are supported by Red Hat.  Since this is a test environment, we will create a pool with EC 2+1 configuration (as we have 3 OSD hosts). This configuration is not recommended and we are using it here only as an example.    To create a EC pool we first need to define the EC scheme by creating a EC profile.   ceph osd erasure-code-profile set my_ec_profile k=2 m=1 ruleset-failure-domain=host   Next create a pool that uses the EC profile we just created   ceph osd pool create ecpool 128 128 erasure my_ec_profile   Verify pool creation   ceph df   Starting RHCS 3, a Pool must be associated with a application, so lets enable application on the newly created pool.   ceph osd pool application enable ecpool benchmarking   Lets write/read some data to/from this pool using  rados bench  tool, which runs a basic benchmarking on a pool.    Tip  As soon as you start generating test data by running following command, switch to Ceph Metrics Dashboard and monitor Ceph cluster activity.   rados bench -p ecpool 60 write  --no-cleanup  rados bench -p ecpool 60 rand   End of Module  We have reached to the end of Module-2. At this point you have learned to deploy, configure and interact with your Ceph cluster together with Ceph Metrics monitoring dashboard. In the next module you will learn Ceph Block storage provisioning.", 
            "title": "Creating a Ceph Storage Pool"
        }, 
        {
            "location": "/Module-3/", 
            "text": "Module - 3 : Ceph Block Storage Provisioning\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be provisioning Ceph Block Storage using the RADOS Block Device (RBD) protocol.\n\n\nLater, you will use RBD to host a MySQL database and perform some basic operations on the database running on Ceph Block Storage.\n\n\nBy the end of this module, you will have an understanding of Ceph Block Storage provisioning.\n\n\n\n\n\n\n\n\nFrom your workstation login to the \nceph-admin\n node as user \nstudent\n \n(Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-admin node\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nThis module is independent of the other modules. If you intend to follow this module, please make sure that you have a running Ceph cluster before you begin.\n\n\nYou could setup a Ceph cluster using either of these two methods\n\n1) Follow the hands-on instructions in Module-2 and deploy the Ceph cluster\n\n2) From \nceph-admin\n node, execute the following script to setup a Ceph cluster \n\n\nsh /home/student/auto-pilot/setup_ceph_cluster.sh\n\n\nOnce you have a running Ceph cluster, you will be ready to continue with this module.\n\n\nYou must run all the commands using user \nstudent\n on the \nceph-admin\n node, unless otherwise specified.\n\n\n\n\n\n\nCeph Block Storage Provisioning through RBD Protocol\n#\n\n\nIn this section you will learn how to provision block storage using Ceph RBD. We will create a thin-provisioned, re-sizable RADOS Block Device (RBD) volume, which you will provision and map to \nclient-node1\n. Later you will use this RBD volume to store a MySQL database.\n\n\n\n\nLet\ns begin by creating a \nrbd\n storage pool\n\n\n\n\nceph osd pool create rbd 64\n\n\n\n\n\n\nAssign an application to this pool\n\n\n\n\nceph osd pool application enable rbd rbd\n\n\n\n\n\n\nCreate a RBD client named \nclient.rbd\n with necessary permissions on Ceph Monitor and OSDs\n\n\n\n\nceph auth get-or-create client.rbd mon \nallow r\n osd \nallow rwx pool=rbd\n -o /etc/ceph/ceph.client.rbd.keyring\n\n\n\n\n\n\nceph-common\n package has already been installed on \nclient-node1\n. Next, set permissions on the \n/etc/ceph\n directory\n\n\n\n\nssh client-node1 -t sudo chown -R ceph:student /etc/ceph\n\n\n\n\nssh client-node1 -t sudo chmod 775 /etc/ceph\n\n\n\n\n\n\nIn order to allow \nclient-node1\n to run Ceph commands, add a copy of the \nceph.conf\n and \nceph.client.rbd.keyring\n files to \nclient-node1\n\n\n\n\nscp /etc/ceph/ceph.conf client-node1:/etc/ceph/ceph.conf\n\n\n\n\nscp /etc/ceph/ceph.client.rbd.keyring client-node1:/etc/ceph\n\n\n\n\n\n\nThe rest of the commands must be run from \nclient-node1\n, so let\ns ssh to client-node1\n\n\n\n\nssh client-node1\n\n\n\n\n\n\nFirst, let\ns verify that \nclient-node1\n is able to access the Ceph cluster through user \nrbd\n \n\n\n\n\nceph -s --id rbd\n\n\n\n\n\n\nNext, we shall provision a RBD volume named \nmariadb-disk1\n of size 10G\n\n\n\n\nrbd create mariadb-disk1 --size 10240 --image-feature layering --id rbd\n\n\n\n\n\n\nVerify the RBD volume that we have just created\n\n\n\n\nrbd ls --id rbd ; \nrbd info mariadb-disk1 --id rbd\n\n\n\n\n\n\nThe Red Hat Linux kernel starting with 2.6.32 comes with native support for Ceph RBD protocol, so let\ns verify that the RBD module is loaded.\n\n\n\n\nlsmod | grep -i rbd\n\n\n\n\n\n\nIf the RBD module is not loaded for some reason, try loading it manually.\n\n\n\n\nsudo modprobe rbd\n\n\n\n\n\n\nOnce the RBD module is loaded, the client node is ready to map the Ceph RBD volume. Let\ns map \nmariadb-disk1\n RBD volume on \nclient-node1\n\n\n\n\nsudo rbd map mariadb-disk1 --id rbd\n\n\n\n\n\n\nVerify that the RBD volume is mapped\n\n\n\n\nrbd showmapped --id rbd\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFrom the above command output, make a note of the OS device name for \nmariadb-disk1\n RBD volume. \n\n\n\n\n\u200b    - In most of the cases it\ns  \n/dev/rbd0\n.\n\n\n\n\n\n\nAt this point, we have created a 10GB RBD volume provisioned and mapped on \nclient-node1\n as a block device. Let\ns make use of this storage by putting a workload on it (MariaDB in our case).\n\n\n\n\n\n\nCreate an xfs file-system and mount the RBD volume on the \n/var/lib/mysql\n directory and verify\n\n\n\n\n\n\nsudo mkfs.xfs /dev/rbd0 ;\nsudo mount /dev/rbd0 /var/lib/mysql ; sudo df -h /var/lib/mysql\n\n\n\n\nNext, we shall configure the MariaDB database and use Ceph RBD for storage.\n\n\n\n\nPrepare the MariaDB configuration file for use\n\n\n\n\nsudo wget https://raw.githubusercontent.com/red-hat-storage/ceph-test-drive-bootstrap/master/mysql-module/my.cnf -O /etc/my.cnf\n\n\n\n\n\n\nTo save time, a MariaDB package has already been installed. Let\ns begin by changing the ownership of the database mount point and start the MariaDB service.\n\n\n\n\nsudo chown -R mysql:mysql /var/lib/mysql\n\n\n\n\nsudo systemctl start mariadb ; sudo systemctl enable mariadb ; sudo systemctl status mariadb\n\n\n\n\n\n\nLogin as \nmysqladmin\n and run a basic query to verify \n\n\n\n\nmysqladmin -u root version\n\n\n\n\nmysql -u root -e \nSHOW DATABASES\n\n\n\n\n\n\n\nIf MariaDB is setup properly, you should see an output similar to that as shown below\n\n\n\n\n[ceph@client-node1 ~]$ mysqladmin -u root version\nmysqladmin  Ver 9.0 Distrib 5.5.52-MariaDB, for Linux on x86_64\nCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.\n\nServer version      5.5.52-MariaDB\nProtocol version    10\nConnection      Localhost via UNIX socket\nUNIX socket     /var/lib/mysql/mysql.sock\nUptime:         41 min 21 sec\n\nThreads: 1  Questions: 1000769  Slow queries: 0  Opens: 65  Flush tables: 2  Open tables: 90  Queries per second avg: 403.373\n[ceph@client-node1 ~]$\n[ceph@client-node1 ~]$\n[ceph@client-node1 ~]$ mysql -u root -e \nSHOW DATABASES\n\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| test               |\n+--------------------+\n[ceph@client-node1 ~]$\n\n\n\n\n\n\nVerify MySQL is using Ceph block storage\n\n\n\n\nsudo ls -l /var/lib/mysql ; sudo df -h /var/lib/mysql\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\nAt this point, we have a running MariaDB database instance on top of Ceph Block Storage. Let\ns try some database read and write operations using \nsysbench\n which is a popular tool for MySQL database benchmarking.\n\n\n\n\n\n\nFor sysbench, create a database user and grant necessary access\n\n\n\n\n\n\nmysql -u root -e \nCREATE DATABASE sysbench;\n \n\n\n\n\nmysql -u root -e \nCREATE USER \nsysbench\n@\nlocalhost\n IDENTIFIED BY \nsecret\n;\n\n\n\n\n\nmysql -u root -e \nGRANT ALL PRIVILEGES ON *.* TO \nsysbench\n@\nlocalhost\n IDENTIFIED  BY \nsecret\n;\n\n\n\n\n\n\n\nGenerate some test data in a \nsysbench\n table.\n\n\n\n\n\n\nTip\n\n\nAs soon as you start generating test data by running the following command, switch to Ceph Metrics Dashboard to monitor your Ceph cluster activity.\n\n\n\n\nsysbench /usr/share/sysbench/oltp_read_write.lua --threads=64 --table_size=2000000 --time=300 --mysql-db=sysbench --mysql-user=sysbench --mysql-password=secret --db-driver=mysql --mysql_storage_engine=innodb prepare\n\n\n\n\n\n\nVerify sysbench created test table rows\n\n\n\n\nmysql -u root -e \nSELECT TABLE_NAME, TABLE_ROWS FROM information_schema.tables WHERE `table_schema` = \nsysbench\n ;\n\n\n\n\n\n\n\nYou will notice a similar output, as shown below\n\n\n\n\n\n\n(click on the screen shot for better resolution)\n\n\n\n\nFinally using Sysbench, simulate read+write (mix) database operations representing a MariaDB workload on Ceph block storage.\n\n\n\n\n\n\nTip\n\n\nAs soon as you start generating test data by running the following command, switch to Ceph Metrics Dashboard to monitor your Ceph cluster activity.\n\n\n\n\nsysbench /usr/share/sysbench/oltp_read_write.lua --table_size=2000000 --time=60 --threads=64 --mysql-db=sysbench --mysql-user=sysbench --mysql-password=secret --db-driver=mysql --mysql_storage_engine=innodb  run\n\n\n\n\n\n\nYou will notice a similar output, as shown below\n\n\n\n\n\n\n(click on the screen shot for better resolution)\n\n\n\n\nImportant\n\n\nBy any means, this test does not represents Ceph or MariaDB actual performance, so do not take these performance numbers literally. This is only for demonstration purposes.\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-3. In this module, you have learned how to provision Ceph block storage through the RBD protocol and have verified its functionality by applying a MariaDB database workload. In the next module, we shall cover Ceph Object Storage.", 
            "title": "Module-3 : Loading Dataset"
        }, 
        {
            "location": "/Module-3/#module-3-ceph-block-storage-provisioning", 
            "text": "Module Agenda   In this module, you will be provisioning Ceph Block Storage using the RADOS Block Device (RBD) protocol.  Later, you will use RBD to host a MySQL database and perform some basic operations on the database running on Ceph Block Storage.  By the end of this module, you will have an understanding of Ceph Block Storage provisioning.     From your workstation login to the  ceph-admin  node as user  student   (Learn how to Login)   ssh student@ IP Address of ceph-admin node    Prerequisite   This module is independent of the other modules. If you intend to follow this module, please make sure that you have a running Ceph cluster before you begin.  You could setup a Ceph cluster using either of these two methods \n1) Follow the hands-on instructions in Module-2 and deploy the Ceph cluster \n2) From  ceph-admin  node, execute the following script to setup a Ceph cluster   sh /home/student/auto-pilot/setup_ceph_cluster.sh  Once you have a running Ceph cluster, you will be ready to continue with this module.  You must run all the commands using user  student  on the  ceph-admin  node, unless otherwise specified.", 
            "title": "Module - 3 : Ceph Block Storage Provisioning"
        }, 
        {
            "location": "/Module-3/#ceph-block-storage-provisioning-through-rbd-protocol", 
            "text": "In this section you will learn how to provision block storage using Ceph RBD. We will create a thin-provisioned, re-sizable RADOS Block Device (RBD) volume, which you will provision and map to  client-node1 . Later you will use this RBD volume to store a MySQL database.   Let s begin by creating a  rbd  storage pool   ceph osd pool create rbd 64   Assign an application to this pool   ceph osd pool application enable rbd rbd   Create a RBD client named  client.rbd  with necessary permissions on Ceph Monitor and OSDs   ceph auth get-or-create client.rbd mon  allow r  osd  allow rwx pool=rbd  -o /etc/ceph/ceph.client.rbd.keyring   ceph-common  package has already been installed on  client-node1 . Next, set permissions on the  /etc/ceph  directory   ssh client-node1 -t sudo chown -R ceph:student /etc/ceph  ssh client-node1 -t sudo chmod 775 /etc/ceph   In order to allow  client-node1  to run Ceph commands, add a copy of the  ceph.conf  and  ceph.client.rbd.keyring  files to  client-node1   scp /etc/ceph/ceph.conf client-node1:/etc/ceph/ceph.conf  scp /etc/ceph/ceph.client.rbd.keyring client-node1:/etc/ceph   The rest of the commands must be run from  client-node1 , so let s ssh to client-node1   ssh client-node1   First, let s verify that  client-node1  is able to access the Ceph cluster through user  rbd     ceph -s --id rbd   Next, we shall provision a RBD volume named  mariadb-disk1  of size 10G   rbd create mariadb-disk1 --size 10240 --image-feature layering --id rbd   Verify the RBD volume that we have just created   rbd ls --id rbd ; \nrbd info mariadb-disk1 --id rbd   The Red Hat Linux kernel starting with 2.6.32 comes with native support for Ceph RBD protocol, so let s verify that the RBD module is loaded.   lsmod | grep -i rbd   If the RBD module is not loaded for some reason, try loading it manually.   sudo modprobe rbd   Once the RBD module is loaded, the client node is ready to map the Ceph RBD volume. Let s map  mariadb-disk1  RBD volume on  client-node1   sudo rbd map mariadb-disk1 --id rbd   Verify that the RBD volume is mapped   rbd showmapped --id rbd   Tip    From the above command output, make a note of the OS device name for  mariadb-disk1  RBD volume.    \u200b    - In most of the cases it s   /dev/rbd0 .    At this point, we have created a 10GB RBD volume provisioned and mapped on  client-node1  as a block device. Let s make use of this storage by putting a workload on it (MariaDB in our case).    Create an xfs file-system and mount the RBD volume on the  /var/lib/mysql  directory and verify    sudo mkfs.xfs /dev/rbd0 ;\nsudo mount /dev/rbd0 /var/lib/mysql ; sudo df -h /var/lib/mysql  Next, we shall configure the MariaDB database and use Ceph RBD for storage.   Prepare the MariaDB configuration file for use   sudo wget https://raw.githubusercontent.com/red-hat-storage/ceph-test-drive-bootstrap/master/mysql-module/my.cnf -O /etc/my.cnf   To save time, a MariaDB package has already been installed. Let s begin by changing the ownership of the database mount point and start the MariaDB service.   sudo chown -R mysql:mysql /var/lib/mysql  sudo systemctl start mariadb ; sudo systemctl enable mariadb ; sudo systemctl status mariadb   Login as  mysqladmin  and run a basic query to verify    mysqladmin -u root version  mysql -u root -e  SHOW DATABASES    If MariaDB is setup properly, you should see an output similar to that as shown below   [ceph@client-node1 ~]$ mysqladmin -u root version\nmysqladmin  Ver 9.0 Distrib 5.5.52-MariaDB, for Linux on x86_64\nCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.\n\nServer version      5.5.52-MariaDB\nProtocol version    10\nConnection      Localhost via UNIX socket\nUNIX socket     /var/lib/mysql/mysql.sock\nUptime:         41 min 21 sec\n\nThreads: 1  Questions: 1000769  Slow queries: 0  Opens: 65  Flush tables: 2  Open tables: 90  Queries per second avg: 403.373\n[ceph@client-node1 ~]$\n[ceph@client-node1 ~]$\n[ceph@client-node1 ~]$ mysql -u root -e  SHOW DATABASES \n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| test               |\n+--------------------+\n[ceph@client-node1 ~]$   Verify MySQL is using Ceph block storage   sudo ls -l /var/lib/mysql ; sudo df -h /var/lib/mysql   Success     At this point, we have a running MariaDB database instance on top of Ceph Block Storage. Let s try some database read and write operations using  sysbench  which is a popular tool for MySQL database benchmarking.    For sysbench, create a database user and grant necessary access    mysql -u root -e  CREATE DATABASE sysbench;    mysql -u root -e  CREATE USER  sysbench @ localhost  IDENTIFIED BY  secret ;   mysql -u root -e  GRANT ALL PRIVILEGES ON *.* TO  sysbench @ localhost  IDENTIFIED  BY  secret ;    Generate some test data in a  sysbench  table.    Tip  As soon as you start generating test data by running the following command, switch to Ceph Metrics Dashboard to monitor your Ceph cluster activity.   sysbench /usr/share/sysbench/oltp_read_write.lua --threads=64 --table_size=2000000 --time=300 --mysql-db=sysbench --mysql-user=sysbench --mysql-password=secret --db-driver=mysql --mysql_storage_engine=innodb prepare   Verify sysbench created test table rows   mysql -u root -e  SELECT TABLE_NAME, TABLE_ROWS FROM information_schema.tables WHERE `table_schema` =  sysbench  ;    You will notice a similar output, as shown below    (click on the screen shot for better resolution)   Finally using Sysbench, simulate read+write (mix) database operations representing a MariaDB workload on Ceph block storage.    Tip  As soon as you start generating test data by running the following command, switch to Ceph Metrics Dashboard to monitor your Ceph cluster activity.   sysbench /usr/share/sysbench/oltp_read_write.lua --table_size=2000000 --time=60 --threads=64 --mysql-db=sysbench --mysql-user=sysbench --mysql-password=secret --db-driver=mysql --mysql_storage_engine=innodb  run   You will notice a similar output, as shown below    (click on the screen shot for better resolution)   Important  By any means, this test does not represents Ceph or MariaDB actual performance, so do not take these performance numbers literally. This is only for demonstration purposes.    End of Module  We have reached the end of Module-3. In this module, you have learned how to provision Ceph block storage through the RBD protocol and have verified its functionality by applying a MariaDB database workload. In the next module, we shall cover Ceph Object Storage.", 
            "title": "Ceph Block Storage Provisioning through RBD Protocol"
        }, 
        {
            "location": "/Module-4/", 
            "text": "Module - 4 : Ceph Object Storage Provisioning\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module you will learn how to configure object storage using the Ceph RADOS Gateway (RGW) protocol\n\n\nYou will then use Ceph for Object Storage over S3 and SWIFT APIs\n\n\nFinally, you will configure a Sree GUI S3 Browser to access Ceph Object Storage via the S3 API\n\n\nBy the end of this module, you will have an understanding of Ceph Object Storage.\n\n\n\n\n\n\n\n\nFrom your workstation login to the \nceph-admin\n node as user \nstudent\n \n(Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-admin node\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nThis module is independent of the other modules. If you intend to follow this module, please make sure that you have a running Ceph cluster before you begin.\n\n\nYou could setup a Ceph cluster using either of these two methods\n\n1) Follow the hands-on instructions in Module-2 and deploy the Ceph cluster\n\n2) From \nceph-admin\n node, execute the following script to setup a Ceph cluster\n\n\nsh /home/student/auto-pilot/setup_ceph_cluster.sh\n\n\nOnce you have a running Ceph cluster you will be ready to continue with this module.\n\n\nYou must run all the commands using user \nstudent\n on the \nceph-admin\n node, unless otherwise specified.\n\n\n\n\n\n\nIntroduction\n#\n\n\nThe Ceph object gateway, also know as the RADOS gateway (RGW), is an object storage interface built on top of the librados API to provide applications with a RESTful gateway to Ceph storage clusters. \n\n\nTo access Ceph over object storage interfaces i.e. via \nswift\n or \ns3\n APIs, we need to configure Ceph RGW. In this module we will configure the \nceph-admin\n node as Ceph RGW and verify \ns3\n and \nswift\n client access from \nclient-node1\n \n\n\nSetting up Ceph Rados Gateway\n#\n\n\n\n\nVisit the \nceph-ansible\n main configuration directory\n\n\n\n\ncd /usr/share/ceph-ansible/group_vars/\n\n\n\n\n\n\nIn this directory you will find the \nall.yml\n configuration file which has \nalready been pre-populated for you\n to avoid any typographical errors. Let\ns review this file to better understand the Ceph RGW configuration parameters.\n\n\n\n\ncat all.yml\n\n\n\n\n\n\nall.yml\n file configures Ceph RGW for\n\n\nPort\n\n\nNumber of concurrent threads\n\n\nRGW interface\n\n\nRGW DNS name (required for s3 style bucket access)\n\n\n\n\n\n\n\n\n...\n\n\n...\n\n\nradosgw_civetweb_port: 80\n\n\nradosgw_civetweb_num_threads: 512\n\n\nradosgw_civetweb_options: \nnum_threads=\n{{\n \nradosgw_civetweb_num_threads\n \n}}\n\n\nradosgw_interface: eth0\n\n\nradosgw_dns_name: \nceph-admin\n\n\n...\n\n\n...\n\n\n\n\n\n\n\nAdd Ceph RGW host to your Ansible inventory file by editing the \n/etc/ansible/hosts\n file\n\n\n\n\nsudo vim /etc/ansible/hosts\n\n\n\n\n\n\nAdd the following section to your \n/etc/ansible/hosts\n file\n\n\n\n\n[rgws]\n\n\nceph-admin\n\n\n\n\n\n\n\nTo start deploying Ceph RGW, switch your directory to the \nceph-ansible\n root\n\n\n\n\ncd /usr/share/ceph-ansible\n\n\n\n\n\n\nRun the \nceph-ansible\n playbook and limit it to \nrgw\n \n\n\n\n\ntime ansible-playbook site.yml --limit  rgws\n\n\n\n\n\n\nTip\n\n\nAnsible is idempotent. If it is run multiple times, it has the same effect as running it once. Therefore, there is no harm in running it again. Configuration changes will not take place after its initial application.\n\n\n\n\n\n\n\n\nOnce your Ansible playbook run has finished, ensure there are no failed items under \nPLAY RECAP\n \n\n\n\n\n\n\nAllow user \nstudent\n to access the Ceph cluster\n\n\n\n\n\n\nsudo chown -R student:student /etc/ceph\n\n\n\n\n\n\nVerify \nceph-radosgw\n service is running on \nceph-admin\n . Also make note of the port number it\ns running on. It must be on port 80.\n\n\n\n\nsudo systemctl status ceph-radosgw@rgw.ceph-admin.service ; \nsudo netstat -plunt | grep -i rados ;\n\n\n\n\n\n\n\n\nTo use Ceph as an Object Storage cluster, we first need to create \nS3\n and \nswift\n users\n\n\n\n\n\n\nCreate a RGW user for \nS3\n access \n\n\n\n\n\n\nsudo radosgw-admin user create --uid=\nuser1\n --display-name=\nFirst User\n --access-key=\nS3user1\n --secret-key=\nS3user1key\n\n\n\n\n\n\n\nCreate RGW subuser for \nswift\n access\n\n\n\n\nsudo radosgw-admin subuser create --uid=\nuser1\n --subuser=\nuser1:swift\n --secret-key=\nSwiftuser1key\n --access=full\n\n\n\n\n\n\nSuccess\n\n\nAt this point, you have Ceph RGW installed and configured, and you have created S3 and Swift users. In the next section, you will learn how to access Ceph object storage using \nS3\n and \nSwift\n clients.\n\n\n\n\nAccess Ceph Object Storage using Swift API\n#\n\n\n\n\n\n\nTo save time, \npython-swiftclient\n cli has already been installed on \nceph-admin\n node\n\n\n\n\n\n\nUsing Swift CLI, create a swift container (aka bucket) named \ncontainer-1\n and then list it\n\n\n\n\n\n\nswift\n \n-A\n \nhttp\n://\nceph-admin\n/\nauth\n/\n1\n.\n0\n  \n-U\n \nuser1\n:\nswift\n \n-K\n \nSwiftuser1key\n \npost\n \ncontainer-1\n \n;\n \n\n\n\n\nswift -A http://ceph-admin/auth/1.0  -U user1:swift -K \nSwiftuser1key\n list\n\n\n\n\n\n\nCreate a dummy file and then upload it to \ncontainer-1\n using swift\n\n\n\n\ncd /tmp ; base64 /dev/urandom | head -c 10000000 \n dummy_file1.txt ; \n\n\n\n\nswift -A http://ceph-admin/auth/1.0  -U user1:swift -K \nSwiftuser1key\n upload container-1 dummy_file1.txt\n\n\n\n\n\n\nList \ncontainer-1\n to verify the file is stored\n\n\n\n\nswift\n \n-A\n \nhttp\n://\nceph-admin\n/\nauth\n/\n1\n.\n0\n  \n-U\n \nuser1\n:\nswift\n \n-K\n \nSwiftuser1key\n \nlist\n \ncontainer-1\n;\n \n\n\n\n\n\n\nSuccess\n\n\nEasy, right? So you have just learned how to use Ceph for Object Storage using swift APIs. \n\n\n\n\nAccess Ceph Object Storage using S3 API\n#\n\n\nThe Ceph object storage cluster can be accessed by any client that supports \nS3\n API access.  In this section we will use \ns3cmd\n which has already been installed on the \nceph-admin\n node.\n\n\n\n\n\n\nTo use Ceph with S3-style subdomains (e.g., bucket-name.domain-name.com), you need to add a wildcard to the DNS record of the DNS server you use with the ceph-radosgw daemon. We will rely on \ndnsmasq\n which is a lightweight DNS server and has already been installed and configured on the \nceph-admin\n node\n\n\n\n\n\n\nNext, we shall configure \ns3cmd\n on \nceph-admin\n node\n\n\n\n\n\n\ns3cmd --access_key=S3user1 --secret_key=S3user1key --no-ssl --host=ceph-admin --host-bucket=\n%(bucket)s.ceph-admin\n --dump-config \n /home/student/.s3cfg\n\n\n\n\n\n\nTest Ceph object storage via S3 protocol by listing buckets using \ns3cmd\n. It should list buckets that you have created using \nswift\n in the last section \n\n\n\n\ns3cmd ls\n\n\n\n\n\n\nCreate a new bucket and list \n\n\n\n\ns3cmd\n \nmb\n \ns3\n://\ns3-bucket\n \n;\n \n\n\n\n\ns3cmd ls\n\n\n\n\n\n\nCreate a dummy file and upload to \ns3-bucket\n via S3 API\n\n\n\n\ncd /tmp ; base64 /dev/urandom | head -c 10000000 \n dummy_file2.txt ;\n\n\n\n\ns3cmd\n \nput\n \ndummy_file2\n.\ntxt\n \ns3\n://\ns3-bucket\n \n;\n \n\n\n\n\ns3cmd ls s3://s3-bucket\n\n\n\n\n\n\nNow, create a publicly accessible Ceph S3 bucket\n\n\n\n\ns3cmd mb s3://public_bucket --acl-public\n\n\n\n\n\n\nAdd content (Image File) to the S3 public bucket\n\n\n\n\ns3cmd put --acl-public  /home/student/Red_Hat_Tower.jpg s3://public_bucket \n\n\n\n\n\n\nAdd content (Video File) to the S3 public bucket\n\n\n\n\ns3cmd put --acl-public /home/student/Red_Hat_Ceph_Storage.mp4 s3://public_bucket\n\n\n\n\n\n\n\n\nNext, let\ns view this public content directly from your browser\n\n\n\n\n\n\nTo view the image file, update the following URL with the public IP address of your \nceph-admin\n node and copy this to your browser\n\n\n\n\n\n\nhttp://\nPublic_IP_of_Ceph_Admin_Node\n/public_bucket/Red_Hat_Tower.jpg\n\n\n\n\n\n\nTo stream video directly from Ceph S3 public bucket, again, update the URL below with the public IP address of your \nceph-admin\n node and copy the result to your browser\n\n\n\n\nhttp://\nPublic_IP_of_Ceph_Admin_Node\n/public_bucket/Red_Hat_Ceph_Storage.mp4\n\n\n\n\n\n\n\n\nFinally, let\ns use an Open Source object storage GUI browser, \nSree\n, to access Ceph Object Storage using the S3 API. For simplicity, Sree has been configured on your \nceph-admin\n node and is running on port \n5000\n.\n\n\n\n\n\n\nTo open the Sree Object Storage browser, update the following URL with your \nceph-admin\n node\ns public IP addresss and point your browser to\n\n\n\n\n\n\nhttp://\nPublic_IP_of_Ceph_Admin_Node\n:5000\n\n\n\n\n\n\nLet\ns configure Sree to access Ceph S3 by providing the following details in the configuration section (as shown in the screen shot)\n\n\n\n\nCeph S3 Endpoint : http://\nPublic_IP_of_Ceph_Admin_Node\n\nAccess Key ID : S3user1\nAccess Key ID : S3user1key\n\n\n\n\n\n\n\n\nOnce you have Sree configured with Ceph S3 endpoints and keys, click on the buckets section to list/create new buckets from the Sree object storage browser\n\n\n\n\n\n\n\n\nNote\n\n\nBrowsing buckets upon which Cross-Origin Resource Sharing (CORS) has not been set could result in a \nNetwork Error\n. To avoid such errors, set CORS on the bucket either through the Sree web interface or the S3cmd CLI. It\ns worth noting that buckets created through the Sree web interface have CORS set by default and are browsable. Learn more about \nCORS\n.\n\n\n\n\n\n\nLet\ns create a new bucket, \nsree-bucket\n , from the Sree Object Storage browser \n\n\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-4. In this module, you have learned to use the Ceph cluster for object storage using S3 and Swift APIs. In the next module, you will learn to use Ceph storage as a distributed file system.", 
            "title": "Module-4 : Lab excercise-1"
        }, 
        {
            "location": "/Module-4/#module-4-ceph-object-storage-provisioning", 
            "text": "Module Agenda   In this module you will learn how to configure object storage using the Ceph RADOS Gateway (RGW) protocol  You will then use Ceph for Object Storage over S3 and SWIFT APIs  Finally, you will configure a Sree GUI S3 Browser to access Ceph Object Storage via the S3 API  By the end of this module, you will have an understanding of Ceph Object Storage.     From your workstation login to the  ceph-admin  node as user  student   (Learn how to Login)   ssh student@ IP Address of ceph-admin node    Prerequisite   This module is independent of the other modules. If you intend to follow this module, please make sure that you have a running Ceph cluster before you begin.  You could setup a Ceph cluster using either of these two methods \n1) Follow the hands-on instructions in Module-2 and deploy the Ceph cluster \n2) From  ceph-admin  node, execute the following script to setup a Ceph cluster  sh /home/student/auto-pilot/setup_ceph_cluster.sh  Once you have a running Ceph cluster you will be ready to continue with this module.  You must run all the commands using user  student  on the  ceph-admin  node, unless otherwise specified.", 
            "title": "Module - 4 : Ceph Object Storage Provisioning"
        }, 
        {
            "location": "/Module-4/#introduction", 
            "text": "The Ceph object gateway, also know as the RADOS gateway (RGW), is an object storage interface built on top of the librados API to provide applications with a RESTful gateway to Ceph storage clusters.   To access Ceph over object storage interfaces i.e. via  swift  or  s3  APIs, we need to configure Ceph RGW. In this module we will configure the  ceph-admin  node as Ceph RGW and verify  s3  and  swift  client access from  client-node1", 
            "title": "Introduction"
        }, 
        {
            "location": "/Module-4/#setting-up-ceph-rados-gateway", 
            "text": "Visit the  ceph-ansible  main configuration directory   cd /usr/share/ceph-ansible/group_vars/   In this directory you will find the  all.yml  configuration file which has  already been pre-populated for you  to avoid any typographical errors. Let s review this file to better understand the Ceph RGW configuration parameters.   cat all.yml   all.yml  file configures Ceph RGW for  Port  Number of concurrent threads  RGW interface  RGW DNS name (required for s3 style bucket access)     ...  ...  radosgw_civetweb_port: 80  radosgw_civetweb_num_threads: 512  radosgw_civetweb_options:  num_threads= {{   radosgw_civetweb_num_threads   }}  radosgw_interface: eth0  radosgw_dns_name:  ceph-admin  ...  ...    Add Ceph RGW host to your Ansible inventory file by editing the  /etc/ansible/hosts  file   sudo vim /etc/ansible/hosts   Add the following section to your  /etc/ansible/hosts  file   [rgws]  ceph-admin    To start deploying Ceph RGW, switch your directory to the  ceph-ansible  root   cd /usr/share/ceph-ansible   Run the  ceph-ansible  playbook and limit it to  rgw     time ansible-playbook site.yml --limit  rgws   Tip  Ansible is idempotent. If it is run multiple times, it has the same effect as running it once. Therefore, there is no harm in running it again. Configuration changes will not take place after its initial application.     Once your Ansible playbook run has finished, ensure there are no failed items under  PLAY RECAP      Allow user  student  to access the Ceph cluster    sudo chown -R student:student /etc/ceph   Verify  ceph-radosgw  service is running on  ceph-admin  . Also make note of the port number it s running on. It must be on port 80.   sudo systemctl status ceph-radosgw@rgw.ceph-admin.service ; \nsudo netstat -plunt | grep -i rados ;    To use Ceph as an Object Storage cluster, we first need to create  S3  and  swift  users    Create a RGW user for  S3  access     sudo radosgw-admin user create --uid= user1  --display-name= First User  --access-key= S3user1  --secret-key= S3user1key    Create RGW subuser for  swift  access   sudo radosgw-admin subuser create --uid= user1  --subuser= user1:swift  --secret-key= Swiftuser1key  --access=full   Success  At this point, you have Ceph RGW installed and configured, and you have created S3 and Swift users. In the next section, you will learn how to access Ceph object storage using  S3  and  Swift  clients.", 
            "title": "Setting up Ceph Rados Gateway"
        }, 
        {
            "location": "/Module-4/#access-ceph-object-storage-using-swift-api", 
            "text": "To save time,  python-swiftclient  cli has already been installed on  ceph-admin  node    Using Swift CLI, create a swift container (aka bucket) named  container-1  and then list it    swift   -A   http :// ceph-admin / auth / 1 . 0    -U   user1 : swift   -K   Swiftuser1key   post   container-1   ;    swift -A http://ceph-admin/auth/1.0  -U user1:swift -K  Swiftuser1key  list   Create a dummy file and then upload it to  container-1  using swift   cd /tmp ; base64 /dev/urandom | head -c 10000000   dummy_file1.txt ;   swift -A http://ceph-admin/auth/1.0  -U user1:swift -K  Swiftuser1key  upload container-1 dummy_file1.txt   List  container-1  to verify the file is stored   swift   -A   http :// ceph-admin / auth / 1 . 0    -U   user1 : swift   -K   Swiftuser1key   list   container-1 ;     Success  Easy, right? So you have just learned how to use Ceph for Object Storage using swift APIs.", 
            "title": "Access Ceph Object Storage using Swift API"
        }, 
        {
            "location": "/Module-4/#access-ceph-object-storage-using-s3-api", 
            "text": "The Ceph object storage cluster can be accessed by any client that supports  S3  API access.  In this section we will use  s3cmd  which has already been installed on the  ceph-admin  node.    To use Ceph with S3-style subdomains (e.g., bucket-name.domain-name.com), you need to add a wildcard to the DNS record of the DNS server you use with the ceph-radosgw daemon. We will rely on  dnsmasq  which is a lightweight DNS server and has already been installed and configured on the  ceph-admin  node    Next, we shall configure  s3cmd  on  ceph-admin  node    s3cmd --access_key=S3user1 --secret_key=S3user1key --no-ssl --host=ceph-admin --host-bucket= %(bucket)s.ceph-admin  --dump-config   /home/student/.s3cfg   Test Ceph object storage via S3 protocol by listing buckets using  s3cmd . It should list buckets that you have created using  swift  in the last section    s3cmd ls   Create a new bucket and list    s3cmd   mb   s3 :// s3-bucket   ;    s3cmd ls   Create a dummy file and upload to  s3-bucket  via S3 API   cd /tmp ; base64 /dev/urandom | head -c 10000000   dummy_file2.txt ;  s3cmd   put   dummy_file2 . txt   s3 :// s3-bucket   ;    s3cmd ls s3://s3-bucket   Now, create a publicly accessible Ceph S3 bucket   s3cmd mb s3://public_bucket --acl-public   Add content (Image File) to the S3 public bucket   s3cmd put --acl-public  /home/student/Red_Hat_Tower.jpg s3://public_bucket    Add content (Video File) to the S3 public bucket   s3cmd put --acl-public /home/student/Red_Hat_Ceph_Storage.mp4 s3://public_bucket    Next, let s view this public content directly from your browser    To view the image file, update the following URL with the public IP address of your  ceph-admin  node and copy this to your browser    http:// Public_IP_of_Ceph_Admin_Node /public_bucket/Red_Hat_Tower.jpg   To stream video directly from Ceph S3 public bucket, again, update the URL below with the public IP address of your  ceph-admin  node and copy the result to your browser   http:// Public_IP_of_Ceph_Admin_Node /public_bucket/Red_Hat_Ceph_Storage.mp4    Finally, let s use an Open Source object storage GUI browser,  Sree , to access Ceph Object Storage using the S3 API. For simplicity, Sree has been configured on your  ceph-admin  node and is running on port  5000 .    To open the Sree Object Storage browser, update the following URL with your  ceph-admin  node s public IP addresss and point your browser to    http:// Public_IP_of_Ceph_Admin_Node :5000   Let s configure Sree to access Ceph S3 by providing the following details in the configuration section (as shown in the screen shot)   Ceph S3 Endpoint : http:// Public_IP_of_Ceph_Admin_Node \nAccess Key ID : S3user1\nAccess Key ID : S3user1key    Once you have Sree configured with Ceph S3 endpoints and keys, click on the buckets section to list/create new buckets from the Sree object storage browser     Note  Browsing buckets upon which Cross-Origin Resource Sharing (CORS) has not been set could result in a  Network Error . To avoid such errors, set CORS on the bucket either through the Sree web interface or the S3cmd CLI. It s worth noting that buckets created through the Sree web interface have CORS set by default and are browsable. Learn more about  CORS .    Let s create a new bucket,  sree-bucket  , from the Sree Object Storage browser      End of Module  We have reached the end of Module-4. In this module, you have learned to use the Ceph cluster for object storage using S3 and Swift APIs. In the next module, you will learn to use Ceph storage as a distributed file system.", 
            "title": "Access Ceph Object Storage using S3 API"
        }, 
        {
            "location": "/Module-5/", 
            "text": "Module - 5 : Ceph File Storage Provisioning\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will learn how to configure Ceph Metadata Server (MDS)\n\n\nYou will then use CephFS to provision, mount and use a shared filesystem based on Ceph\n\n\nBy the end of this module, you will have an understanding of Ceph Filesystem.\n\n\n\n\n\n\n\n\nFrom your workstation, login to the \nceph-admin\n node as user \nstudent\n \n(Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-admin node\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nThis module is independent of the other modules. If you intend to follow this module, please make sure that you have a running Ceph cluster before you begin.\n\n\nYou could setup a Ceph cluster using either of these two methods\n\n1) Follow the hands-on instructions in Module-2 and deploy the Ceph cluster\n\n2) From \nceph-admin\n node, execute the following script to setup a Ceph cluster\n\n\nsh /home/student/auto-pilot/setup_ceph_cluster.sh\n\n\nOnce you have a running Ceph cluster you will be ready to continue with this module.\n\n\nYou must run all the commands using user \nstudent\n on the \nceph-admin\n node, unless otherwise specified.\n\n\n\n\n\n\nIntroduction\n#\n\n\nThe Ceph File System (CephFS) is a file system compatible with POSIX standards that provides file access to a Ceph Storage Cluster. CephFS requires at least one Metadata Server (MDS) daemon. The MDS daemon manages metadata related to files stored on the Ceph File System and also coordinates access to the shared Ceph Storage Cluster.\n\n\nDeploying Ceph Metadata Server (MDS)\n#\n\n\n\n\n\n\nIn most cases, the default settings for MDS in \nceph-ansible\n are appropriate. To setup MDS, we just need to update the Ansible inventory file with MDS host detail\n\n\n\n\n\n\nEdit Ansible inventory file \n/etc/ansible/hosts\n \n\n\n\n\n\n\nsudo vim /etc/ansible/hosts\n\n\n\n\n\n\nAdd the following section, save and exit from file editor\n\n\n\n\n[mdss]\n\n\nceph-admin\n\n\n\n\n\n\n\nSwitch directory to the \nceph-ansible\n root\n\n\n\n\ncd /usr/share/ceph-ansible\n\n\n\n\n\n\nRun the \nceph-ansible\n playbook and limit it to \nmdss\n \n\n\n\n\ntime ansible-playbook site.yml --limit mdss\n\n\n\n\n\n\nGive user \nstudent\n access to the Ceph CLI \n\n\n\n\nsudo chown -R student:student /etc/ceph\n\n\n\n\n\n\nTip\n\n\nAnsible is idempotent. If it is run multiple times, it has the same effect as running it once. Therefore, there is no harm in running it again. Configuration changes will not take place after its initial application.\n\n\n\n\n\n\nOnce your Ansible playbook run has finished, ensure there are no failed items under \nPLAY RECAP\n \n\n\n\n\nSetting up Ceph Filesystem\n#\n\n\n\n\nCreate a CephFS data pool\n\n\n\n\nceph osd pool create cephfs-data 64\n\n\n\n\n\n\nCreate a CephFS meta data pool\n\n\n\n\nceph osd pool create cephfs-metadata 64\n\n\n\n\n\n\nCreate Ceph Filesystem\n\n\n\n\nceph fs new cephfs cephfs-metadata cephfs-data\n\n\n\n\n\n\nEnable the cephfs application for Ceph data and metadata pools\n\n\n\n\nceph osd pool application enable cephfs-data cephfs ; ceph osd pool application enable cephfs-metadata cephfs ; \n\n\n\n\n\n\nCheck the Ceph Filesystem status\n\n\n\n\nceph fs status cephfs\n\n\n\n\n\n\nCreate a CephFS client user named \nclient.cephfs\n with necessary permissions on Ceph MON, MDS and OSD\n\n\n\n\nceph auth get-or-create client.cephfs mon \nallow r\n mds \nallow rw\n osd \nallow rwx pool=cephfs-metadata,allow rwx pool=cephfs-data\n\n\n\n\n\nAccessing Ceph Filesystem\n#\n\n\n\n\nTo access CephFS, you will need to extract and place a CephFS secret key on to the client node\n\n\n\n\nceph auth get client.cephfs | grep -i key | cut -d \n \n -f 3 \n /tmp/ceph.client.cephfs.secret\n\n\n\n\n\n\nCopy the CephFS secret file to \nclient-node1\n \n\n\n\n\nscp\n \n/\ntmp\n/\nceph\n.\nclient\n.\ncephfs\n.\nsecret\n \nclient-node1\n:\n \n;\n\n\n\n\n\n\n\nConnect to \nclient-node1\n in order to mount the Ceph Filesystem\n\n\n\n\nssh client-node1\n\n\n\n\n\n\nSwitch to \nroot\n user\n\n\n\n\nsudo su -\n\n\n\n\n\n\nCreate a mount point \n\n\n\n\nmkdir /mnt/cephfs\n\n\n\n\n\n\nThen, mount the Ceph Filesystem using the \nmount\n command and add an option, \n-o\n, providing your CephFS secret file\n\n\n\n\nmount -t ceph 10.100.0.11:6789,10.100.0.12:6789,10.100.0.13:6789:/ /mnt/cephfs/ -o name=cephfs,secretfile=/home/student/ceph.client.cephfs.secret\n\n\n\n\n\n\nVerify that the Ceph Filesystem is mounted\n\n\n\n\ndf -h /mnt/cephfs\n\n\n\n\n\n\nNext, let\ns copy a medium-sized file to the Ceph Filesystem\n\n\n\n\ncp /home/student/rhceph-3.0-rhel-7-x86_64.iso /mnt/cephfs/\n\n\n\n\n\n\nVerify the copied files\n\n\n\n\nls -l /mnt/cephfs ; df -h /mnt/cephfs\n\n\n\n\nYou can also mount the same Ceph Filesystem on other nodes in the cluster. As an example, let\ns mount cephfs to the \nceph-admin\n node as well\n\n\n\n\nLogin to \nceph-admin\n node , switch to root user \nsudo su -\n and create mount point for cephfs\n\n\n\n\nssh ceph-admin\n\n\n\n\nsudo su -\n\n\n\n\nmkdir /mnt/cephfs\n\n\n\n\n\n\nAgain, mount the Ceph Filesystem as you did in the previous example, using the \nmount\n command and adding an option, \n-o\n, to provide your CephFS secret file\n\n\n\n\nmount -t ceph 10.100.0.11:6789,10.100.0.12:6789,10.100.0.13:6789:/ /mnt/cephfs/ -o name=cephfs,secretfile=/tmp/ceph.client.cephfs.secret\n\n\n\n\n\n\nVerify the contents of Ceph Filesystem. As expected, the contents will be same as we saw on \nclient-node1\n\n\n\n\nls -l  /mnt/cephfs\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-5. In this module, you have learned how to install, configure and use Ceph as a distributed shared filesystem. In the next module, you will learn Ceph Administration.", 
            "title": "Module-5 : Lab excercise-2"
        }, 
        {
            "location": "/Module-5/#module-5-ceph-file-storage-provisioning", 
            "text": "Module Agenda   In this module, you will learn how to configure Ceph Metadata Server (MDS)  You will then use CephFS to provision, mount and use a shared filesystem based on Ceph  By the end of this module, you will have an understanding of Ceph Filesystem.     From your workstation, login to the  ceph-admin  node as user  student   (Learn how to Login)   ssh student@ IP Address of ceph-admin node    Prerequisite   This module is independent of the other modules. If you intend to follow this module, please make sure that you have a running Ceph cluster before you begin.  You could setup a Ceph cluster using either of these two methods \n1) Follow the hands-on instructions in Module-2 and deploy the Ceph cluster \n2) From  ceph-admin  node, execute the following script to setup a Ceph cluster  sh /home/student/auto-pilot/setup_ceph_cluster.sh  Once you have a running Ceph cluster you will be ready to continue with this module.  You must run all the commands using user  student  on the  ceph-admin  node, unless otherwise specified.", 
            "title": "Module - 5 : Ceph File Storage Provisioning"
        }, 
        {
            "location": "/Module-5/#introduction", 
            "text": "The Ceph File System (CephFS) is a file system compatible with POSIX standards that provides file access to a Ceph Storage Cluster. CephFS requires at least one Metadata Server (MDS) daemon. The MDS daemon manages metadata related to files stored on the Ceph File System and also coordinates access to the shared Ceph Storage Cluster.", 
            "title": "Introduction"
        }, 
        {
            "location": "/Module-5/#deploying-ceph-metadata-server-mds", 
            "text": "In most cases, the default settings for MDS in  ceph-ansible  are appropriate. To setup MDS, we just need to update the Ansible inventory file with MDS host detail    Edit Ansible inventory file  /etc/ansible/hosts      sudo vim /etc/ansible/hosts   Add the following section, save and exit from file editor   [mdss]  ceph-admin    Switch directory to the  ceph-ansible  root   cd /usr/share/ceph-ansible   Run the  ceph-ansible  playbook and limit it to  mdss     time ansible-playbook site.yml --limit mdss   Give user  student  access to the Ceph CLI    sudo chown -R student:student /etc/ceph   Tip  Ansible is idempotent. If it is run multiple times, it has the same effect as running it once. Therefore, there is no harm in running it again. Configuration changes will not take place after its initial application.    Once your Ansible playbook run has finished, ensure there are no failed items under  PLAY RECAP", 
            "title": "Deploying Ceph Metadata Server (MDS)"
        }, 
        {
            "location": "/Module-5/#setting-up-ceph-filesystem", 
            "text": "Create a CephFS data pool   ceph osd pool create cephfs-data 64   Create a CephFS meta data pool   ceph osd pool create cephfs-metadata 64   Create Ceph Filesystem   ceph fs new cephfs cephfs-metadata cephfs-data   Enable the cephfs application for Ceph data and metadata pools   ceph osd pool application enable cephfs-data cephfs ; ceph osd pool application enable cephfs-metadata cephfs ;    Check the Ceph Filesystem status   ceph fs status cephfs   Create a CephFS client user named  client.cephfs  with necessary permissions on Ceph MON, MDS and OSD   ceph auth get-or-create client.cephfs mon  allow r  mds  allow rw  osd  allow rwx pool=cephfs-metadata,allow rwx pool=cephfs-data", 
            "title": "Setting up Ceph Filesystem"
        }, 
        {
            "location": "/Module-5/#accessing-ceph-filesystem", 
            "text": "To access CephFS, you will need to extract and place a CephFS secret key on to the client node   ceph auth get client.cephfs | grep -i key | cut -d     -f 3   /tmp/ceph.client.cephfs.secret   Copy the CephFS secret file to  client-node1     scp   / tmp / ceph . client . cephfs . secret   client-node1 :   ;    Connect to  client-node1  in order to mount the Ceph Filesystem   ssh client-node1   Switch to  root  user   sudo su -   Create a mount point    mkdir /mnt/cephfs   Then, mount the Ceph Filesystem using the  mount  command and add an option,  -o , providing your CephFS secret file   mount -t ceph 10.100.0.11:6789,10.100.0.12:6789,10.100.0.13:6789:/ /mnt/cephfs/ -o name=cephfs,secretfile=/home/student/ceph.client.cephfs.secret   Verify that the Ceph Filesystem is mounted   df -h /mnt/cephfs   Next, let s copy a medium-sized file to the Ceph Filesystem   cp /home/student/rhceph-3.0-rhel-7-x86_64.iso /mnt/cephfs/   Verify the copied files   ls -l /mnt/cephfs ; df -h /mnt/cephfs  You can also mount the same Ceph Filesystem on other nodes in the cluster. As an example, let s mount cephfs to the  ceph-admin  node as well   Login to  ceph-admin  node , switch to root user  sudo su -  and create mount point for cephfs   ssh ceph-admin  sudo su -  mkdir /mnt/cephfs   Again, mount the Ceph Filesystem as you did in the previous example, using the  mount  command and adding an option,  -o , to provide your CephFS secret file   mount -t ceph 10.100.0.11:6789,10.100.0.12:6789,10.100.0.13:6789:/ /mnt/cephfs/ -o name=cephfs,secretfile=/tmp/ceph.client.cephfs.secret   Verify the contents of Ceph Filesystem. As expected, the contents will be same as we saw on  client-node1   ls -l  /mnt/cephfs   End of Module  We have reached the end of Module-5. In this module, you have learned how to install, configure and use Ceph as a distributed shared filesystem. In the next module, you will learn Ceph Administration.", 
            "title": "Accessing Ceph Filesystem"
        }
    ]
}