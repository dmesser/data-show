{
    "docs": [
        {
            "location": "/", 
            "text": "Getting Started\n#\n\n\n\n\nTip\n\n\n\n\nTo open this lab guide in a separate browser window use \nThis Link\n.\n\n\n\n\n\n\nTest Drive Prerequisites\n#\n\n\nFor this lab you need the following:\n\n\n\n\nWorkstation with Internet access\n\n\nSSH client program installed on your workstation\n\n\n\n\nStarting the LAB\n#\n\n\nAfter you have logged into \nRed Hat Test Drive Portal\n , select \nCeph Data Show : All-in-One\n Test Drive from the catalog Icon.\n\n\n\n\nTo start the Test Drive press \n button in the top bar.\n\n\nBy the time your lab environment is getting ready, you could setup Access Keys as shown below.\n\n\n\n\nAccessing the LAB\n#\n\n\n\n\nImmediately after pressing the \nSTART\n button button, the download links for the SSH keys will become active (blue) in the left hand pane in a section labeled \nConnection Details\n:\n\n\n\n\n\n\n\n\n\n\nDownload the PEM key to your computer if you are using regular OpenSSH on the command line with Linux or macOS. Choose Download PPK if you are using PuTTY on Windows.\n\n\n\n\n\n\nChange permission of the downloaded SSH private key\n\n\n\n\n\n\nchmod 400 ~/Dowloads/access_key.pem\n\n\n\n\nWait for lab provisioning to complete\n#\n\n\nGenerally it takes less than 15 minutes to provision your lab environment. Once your lab environment is provisioned, you will find login details on the left hand pane in a section labeled \nConnection Details\n \n\n\n\n\nCeph Node IP Address\n\n\nCeph Node SSH user name\n\n\nCeph S3 Endpoint\n\n\nOpenShift Console URL\n\n\nOpenShift Console user name\n\n\nOpenShift Console Password\n\n\nOpenShift Master Node IP Address\n\n\nOpenShift Master Node SSH user name", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Tip   To open this lab guide in a separate browser window use  This Link .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#test-drive-prerequisites", 
            "text": "For this lab you need the following:   Workstation with Internet access  SSH client program installed on your workstation", 
            "title": "Test Drive Prerequisites"
        }, 
        {
            "location": "/#starting-the-lab", 
            "text": "After you have logged into  Red Hat Test Drive Portal  , select  Ceph Data Show : All-in-One  Test Drive from the catalog Icon.   To start the Test Drive press   button in the top bar.  By the time your lab environment is getting ready, you could setup Access Keys as shown below.", 
            "title": "Starting the LAB"
        }, 
        {
            "location": "/#accessing-the-lab", 
            "text": "Immediately after pressing the  START  button button, the download links for the SSH keys will become active (blue) in the left hand pane in a section labeled  Connection Details :      Download the PEM key to your computer if you are using regular OpenSSH on the command line with Linux or macOS. Choose Download PPK if you are using PuTTY on Windows.    Change permission of the downloaded SSH private key    chmod 400 ~/Dowloads/access_key.pem", 
            "title": "Accessing the LAB"
        }, 
        {
            "location": "/#wait-for-lab-provisioning-to-complete", 
            "text": "Generally it takes less than 15 minutes to provision your lab environment. Once your lab environment is provisioned, you will find login details on the left hand pane in a section labeled  Connection Details     Ceph Node IP Address  Ceph Node SSH user name  Ceph S3 Endpoint  OpenShift Console URL  OpenShift Console user name  OpenShift Console Password  OpenShift Master Node IP Address  OpenShift Master Node SSH user name", 
            "title": "Wait for lab provisioning to complete"
        }, 
        {
            "location": "/Module-1/", 
            "text": "Module - 1 : Ceph cluster \n Object Storage Setup\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module you will be deploying Red Hat Ceph Storage 3 cluster across 3 nodes using Ansible based deployer called \nceph-ansible\n.\n\n\nYou will also learn how to configure object storage for S3 API by setting up Ceph Rados Gateway (RGW)\n\n\n\n\n\n\n\n\nFrom your workstation login to the \nceph-node1\n node with the user name \nstudent\n \n(Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-node1\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou must run all the commands logged in as user \nstudent\n on the \nceph-node1\n node, unless otherwise specified. \n\n\n\n\n\n\nFast Forward Deployment\n#\n\n\nIn order to save your precious lab time, this section deploys and configures the Ceph Cluster as well as Ceph S3 Object storage in a highly automated way using a all-in-one shell script. If you are using this method of deployment, you could skip the next sections labeled as \nManual Deployment\n\n\n\n\nTo start \nFast Forward Deployer\n run the following command\n\n\n\n\nsh /home/student/auto-pilot/setup_ceph_cluster_with_rgw.sh\n\n\n\n\nManual Deployment : Setting up environment for ceph-ansible\n#\n\n\n\n\nInfo\n\n\n\n\nIf you have choose to follow the above \nFast Forward Deployment\n method, you should skip the below \nManual Deployment\n process.\n\n\n\n\n\n\n\n\nBegin by creating a directory for ceph-ansible keys under \nstudent\n user\ns home directory.\n\n\n\n\nmkdir ~/ceph-ansible-keys\n\n\n\n\n\n\nCreate a new ansible inventory file which helps \nceph-ansible\n to know what role needs to be applied on each node.\n\n\n\n\nsudo vi /etc/ansible/hosts\n\n\n\n\n\n\nIn the \n/etc/ansible/hosts\n inventory file add the following\n\n\n\n\n[mons]\n\n\nceph-node[1:3]\n\n\n\n[osds]\n\n\nceph-node[1:3]\n\n\n\n[mgrs]\n\n\nceph-node1\n\n\n\n[clients]\n\n\nceph-node1\n\n\n\n[rgws]\n\n\nceph-node1\n\n\n\n\n\n\n\nInfo\n\n\n\n\nSince this is a lab environment we are collocating Ceph Mon and Ceph OSD daemons on \nceph-node*\n nodes\n\n\nAlso \nceph-node1\n node will host Ceph Client, Ceph Manager and Ceph RGW services\n\n\n\n\n\n\n\n\nBefore we begin Ceph deployment, make sure that Ansible can reach to all the cluster nodes.\n\n\n\n\nansible all -m ping\n\n\n\n\nManual Deployment : Configuring Ceph-Ansible Settings\n#\n\n\n\n\nVisit \nceph-ansible\n main configuration directory\n\n\n\n\ncd /usr/share/ceph-ansible/group_vars/\n\n\n\n\n\n\nIn the directory you will find \nall.yml\n , \nosds.yml\n and \nclients.yml\n configuration files which are \npre-populated for you\n to avoid any typographic errors. Lets look at these configuration files one by one.\n\n\n\n\n\n\nTip\n\n\nYou can skip editing configuration files as they are pre-populated with correct settings to avoid typos and save time.\n\n\n\n\n\n\nall.yml\n configuration file most importantly configures\n\n\nCeph repository, path to RHCS ISO\n\n\nCeph Monitor network interface ID, public network\n\n\nCeph OSD backend as \nfilestore\n\n\nCeph RGW port, threads and interface\n\n\nCeph configuration settings for pools\n\n\n\n\n\n\n\n\ncat all.yml\n\n\n\n\n---\n\n\ndummy:\n\n\nfetch_directory: ~/ceph-ansible-keys\n\n\nceph_repository_type: iso\n\n\nceph_origin: repository\n\n\nceph_repository: rhcs\n\n\nceph_rhcs_version: 3\n\n\nceph_rhcs_iso_path: \n/home/student/rhceph-3.0-rhel-7-x86_64.iso\n\n\n\nmonitor_interface: eth0\n\n\nmon_use_fqdn: true\n\n\npublic_network: 10.0.1.0/24\n\n\nosd_objectstore: filestore\n\n\n\nradosgw_civetweb_port: 80\n\n\nradosgw_civetweb_num_threads: 512\n\n\nradosgw_civetweb_options: \nnum_threads=\n{{\n \nradosgw_civetweb_num_threads\n \n}}\n\n\nradosgw_interface: eth0\n\n\nradosgw_dns_name: \nceph-node1\n\n\n\nceph_conf_overrides:\n\n\n  global:\n\n\n    osd pool default pg num: 64\n\n\n    osd pool default pgp num: 64\n\n\n    mon allow pool delete: true\n\n\n    mon clock drift allowed: 5\n\n\n    rgw dns name: \nceph-node1\n\n\n\n\n\n\n\nosds.yml\n configuration file most importantly configures\n\n\nCeph OSD deployment scenario to be collocated (ceph-data and ceph-journal on same device)\n\n\nAuto discover storage device and use them as Ceph OSD\n\n\nAllow Ceph OSD nodes to be ceph admin nodes (optional)\n\n\n\n\n\n\n\n\ncat osds.yml\n\n\n\n\n---\ndummy:\ncopy_admin_key: true\nosd_auto_discovery: true\nosd_scenario: collocated\n\n\n\n\n\n\nclients.yml\n configuration file most importantly configures\n\n\nAllow Ceph client nodes to issue ceph admin commands (optional, not recomended for production)\n\n\n\n\n\n\n\n\ncat clients.yml\n\n\n\n\n---\ndummy:\ncopy_admin_key: True\n\n\n\n\nManual Deployment of RHCS Cluster\n#\n\n\n\n\nTo start deploying RHCS cluster, switch to \nceph-ansible\n root directory\n\n\n\n\ncd /usr/share/ceph-ansible\n\n\n\n\n\n\nRun \nceph-ansible\n playbook\n\n\n\n\ntime ansible-playbook site.yml\n\n\n\n\n\n\nThis should usually take 10-12 minutes to complete. Once its done, play recap should look similar to below. Make sure play recap does not show any host run failed.\n\n\n\n\nPLAY RECAP ******************************************************************\nceph-node1                 : ok=149  changed=29   unreachable=0    failed=0\nceph-node2                 : ok=136  changed=24   unreachable=0    failed=0\nceph-node3                 : ok=138  changed=25   unreachable=0    failed=0\n\nreal  10m9.966s\nuser  2m6.029s\nsys 1m1.005s\n\n\n\n\n\n\nFinally check the status of your cluster. \n\n\n\n\nsudo ceph -s\n\n\n\n\n[student@ceph-node1 ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-node1(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-node1 ceph-ansible]$\n\n\n\n\n\n\nSuccess\n\n\nAt this point you should have a healthy RHCS cluster up and running with 3 x Ceph Monitors, 3 x Ceph OSDs (12 x OSDs), 1 x Ceph Manager , 1 x Ceph RGW.\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached to the end of Module-1. At this point you have learned to deploy a Ceph cluster as well as Ceph S3 Object Storage.", 
            "title": "Module-1 : Ceph cluster & Object Storage Setup"
        }, 
        {
            "location": "/Module-1/#module-1-ceph-cluster-object-storage-setup", 
            "text": "Module Agenda   In this module you will be deploying Red Hat Ceph Storage 3 cluster across 3 nodes using Ansible based deployer called  ceph-ansible .  You will also learn how to configure object storage for S3 API by setting up Ceph Rados Gateway (RGW)     From your workstation login to the  ceph-node1  node with the user name  student   (Learn how to Login)   ssh student@ IP Address of ceph-node1    Prerequisite   You must run all the commands logged in as user  student  on the  ceph-node1  node, unless otherwise specified.", 
            "title": "Module - 1 : Ceph cluster &amp; Object Storage Setup"
        }, 
        {
            "location": "/Module-1/#fast-forward-deployment", 
            "text": "In order to save your precious lab time, this section deploys and configures the Ceph Cluster as well as Ceph S3 Object storage in a highly automated way using a all-in-one shell script. If you are using this method of deployment, you could skip the next sections labeled as  Manual Deployment   To start  Fast Forward Deployer  run the following command   sh /home/student/auto-pilot/setup_ceph_cluster_with_rgw.sh", 
            "title": "Fast Forward Deployment"
        }, 
        {
            "location": "/Module-1/#manual-deployment-setting-up-environment-for-ceph-ansible", 
            "text": "Info   If you have choose to follow the above  Fast Forward Deployment  method, you should skip the below  Manual Deployment  process.     Begin by creating a directory for ceph-ansible keys under  student  user s home directory.   mkdir ~/ceph-ansible-keys   Create a new ansible inventory file which helps  ceph-ansible  to know what role needs to be applied on each node.   sudo vi /etc/ansible/hosts   In the  /etc/ansible/hosts  inventory file add the following   [mons]  ceph-node[1:3]  [osds]  ceph-node[1:3]  [mgrs]  ceph-node1  [clients]  ceph-node1  [rgws]  ceph-node1    Info   Since this is a lab environment we are collocating Ceph Mon and Ceph OSD daemons on  ceph-node*  nodes  Also  ceph-node1  node will host Ceph Client, Ceph Manager and Ceph RGW services     Before we begin Ceph deployment, make sure that Ansible can reach to all the cluster nodes.   ansible all -m ping", 
            "title": "Manual Deployment : Setting up environment for ceph-ansible"
        }, 
        {
            "location": "/Module-1/#manual-deployment-configuring-ceph-ansible-settings", 
            "text": "Visit  ceph-ansible  main configuration directory   cd /usr/share/ceph-ansible/group_vars/   In the directory you will find  all.yml  ,  osds.yml  and  clients.yml  configuration files which are  pre-populated for you  to avoid any typographic errors. Lets look at these configuration files one by one.    Tip  You can skip editing configuration files as they are pre-populated with correct settings to avoid typos and save time.    all.yml  configuration file most importantly configures  Ceph repository, path to RHCS ISO  Ceph Monitor network interface ID, public network  Ceph OSD backend as  filestore  Ceph RGW port, threads and interface  Ceph configuration settings for pools     cat all.yml  ---  dummy:  fetch_directory: ~/ceph-ansible-keys  ceph_repository_type: iso  ceph_origin: repository  ceph_repository: rhcs  ceph_rhcs_version: 3  ceph_rhcs_iso_path:  /home/student/rhceph-3.0-rhel-7-x86_64.iso  monitor_interface: eth0  mon_use_fqdn: true  public_network: 10.0.1.0/24  osd_objectstore: filestore  radosgw_civetweb_port: 80  radosgw_civetweb_num_threads: 512  radosgw_civetweb_options:  num_threads= {{   radosgw_civetweb_num_threads   }}  radosgw_interface: eth0  radosgw_dns_name:  ceph-node1  ceph_conf_overrides:    global:      osd pool default pg num: 64      osd pool default pgp num: 64      mon allow pool delete: true      mon clock drift allowed: 5      rgw dns name:  ceph-node1    osds.yml  configuration file most importantly configures  Ceph OSD deployment scenario to be collocated (ceph-data and ceph-journal on same device)  Auto discover storage device and use them as Ceph OSD  Allow Ceph OSD nodes to be ceph admin nodes (optional)     cat osds.yml  ---\ndummy:\ncopy_admin_key: true\nosd_auto_discovery: true\nosd_scenario: collocated   clients.yml  configuration file most importantly configures  Allow Ceph client nodes to issue ceph admin commands (optional, not recomended for production)     cat clients.yml  ---\ndummy:\ncopy_admin_key: True", 
            "title": "Manual Deployment : Configuring Ceph-Ansible Settings"
        }, 
        {
            "location": "/Module-1/#manual-deployment-of-rhcs-cluster", 
            "text": "To start deploying RHCS cluster, switch to  ceph-ansible  root directory   cd /usr/share/ceph-ansible   Run  ceph-ansible  playbook   time ansible-playbook site.yml   This should usually take 10-12 minutes to complete. Once its done, play recap should look similar to below. Make sure play recap does not show any host run failed.   PLAY RECAP ******************************************************************\nceph-node1                 : ok=149  changed=29   unreachable=0    failed=0\nceph-node2                 : ok=136  changed=24   unreachable=0    failed=0\nceph-node3                 : ok=138  changed=25   unreachable=0    failed=0\n\nreal  10m9.966s\nuser  2m6.029s\nsys 1m1.005s   Finally check the status of your cluster.    sudo ceph -s  [student@ceph-node1 ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-node1(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-node1 ceph-ansible]$   Success  At this point you should have a healthy RHCS cluster up and running with 3 x Ceph Monitors, 3 x Ceph OSDs (12 x OSDs), 1 x Ceph Manager , 1 x Ceph RGW.    End of Module  We have reached to the end of Module-1. At this point you have learned to deploy a Ceph cluster as well as Ceph S3 Object Storage.", 
            "title": "Manual Deployment of RHCS Cluster"
        }, 
        {
            "location": "/Module-2/", 
            "text": "Module - 2 : Setting up OpenShift Container Platform\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module you will be creating an OpenShift project required for the rest of the sessions\n\n\n\n\n\n\n\n\nFrom your workstation login to the \nOpenShift Master Node\n with the user name \ncloud-user\n and \nSSH Private Key\n \n(Learn how to Login)\n\n\n\n\nchmod 400 \npath to ssh_key.pem\n\nssh -i \npath to ssh_key.pem\n cloud-user@\nOpenShift Master Node IP Address\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou must run all the commands logged in as user \ncloud-user\n on the \nOpenShift Master Node\n node, unless otherwise specified. \n\n\n\n\n\n\nCreate an OpensShift Project\n#\n\n\nIn order to save your precious lab time, OpenShift Container Platform has already been installed and configured. Before you begin with some data science exercises, lets\n create a OpenShift project\n\n\n\n\nLogin to OpenShift Master Node as \ncloud-user\n\n\n\n\nssh -i \npath to ssh_key.pem\n cloud-user@\nOpenShift Master Node IP Address\n\n\n\n\n\n\n\nLogin to OpenShift\n\n\n\n\noc login -u teamuser1 -p openshift\n\n\n\n\n\n\nCreate a new project\n\n\n\n\noc new-project jupyterhub\n\n\n\n\n\n\nPull the required container image\n\n\n\n\noc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/notebooks.json\n\n\n\n\n\n\nDeploy JupyterHub application\n\n\n\n\noc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/templates.json\n\n\n\n\n\n\nProcess JupyterHub application\n\n\n\n\noc process jupyterhub-ocp-oauth HASH_AUTHENTICATOR_SECRET_KEY=\nmeh\n | oc apply -f -\n\n\n\n\n\n\n\n\nYou should now have JupyterHub pods and services coming up ( will take some time to fully start)\n\n\noc get pods\n\n\n\n\n\n\n\nYou could also monitor your application from OpenShift Container Platform Console by visiting \nOpenShift Console URL\n. The user name and password to access the console is \nteamuser1\n and \nopenshift\n respectively.\n\n\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached to the end of Module-2. At this point you have learned how to deploy an application on OCP. In the later modules we will use this application to perform some interesting data analytics work", 
            "title": "Module-2 : Setting up OpenShift Container Platform"
        }, 
        {
            "location": "/Module-2/#module-2-setting-up-openshift-container-platform", 
            "text": "Module Agenda   In this module you will be creating an OpenShift project required for the rest of the sessions     From your workstation login to the  OpenShift Master Node  with the user name  cloud-user  and  SSH Private Key   (Learn how to Login)   chmod 400  path to ssh_key.pem \nssh -i  path to ssh_key.pem  cloud-user@ OpenShift Master Node IP Address    Prerequisite   You must run all the commands logged in as user  cloud-user  on the  OpenShift Master Node  node, unless otherwise specified.", 
            "title": "Module - 2 : Setting up OpenShift Container Platform"
        }, 
        {
            "location": "/Module-2/#create-an-opensshift-project", 
            "text": "In order to save your precious lab time, OpenShift Container Platform has already been installed and configured. Before you begin with some data science exercises, lets  create a OpenShift project   Login to OpenShift Master Node as  cloud-user   ssh -i  path to ssh_key.pem  cloud-user@ OpenShift Master Node IP Address    Login to OpenShift   oc login -u teamuser1 -p openshift   Create a new project   oc new-project jupyterhub   Pull the required container image   oc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/notebooks.json   Deploy JupyterHub application   oc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/templates.json   Process JupyterHub application   oc process jupyterhub-ocp-oauth HASH_AUTHENTICATOR_SECRET_KEY= meh  | oc apply -f -    You should now have JupyterHub pods and services coming up ( will take some time to fully start)  oc get pods    You could also monitor your application from OpenShift Container Platform Console by visiting  OpenShift Console URL . The user name and password to access the console is  teamuser1  and  openshift  respectively.     End of Module  We have reached to the end of Module-2. At this point you have learned how to deploy an application on OCP. In the later modules we will use this application to perform some interesting data analytics work", 
            "title": "Create an OpensShift Project"
        }, 
        {
            "location": "/Module-3/", 
            "text": "Module - 3 : Introduction to S3A \n Loading Sample Data Set\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be introduced to the S3A filesystem client\n\n\nDownload a sample data set of operational data.\n\n\nCreate a bucket and ingest the local data set to your Ceph object store\n\n\n\n\n\n\n\n\nFrom your workstation, login to the \nceph-node1\n node as user \nstudent\n \n(Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-node1 node\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou need to have completed Modules 1-3 before beginning this module\n\n\n\n\n\n\nIntroduction\n#\n\n\nThe S3A filesystem client comes from Apache Hadoop and can be utilized by Spark and other tools to interact with a S3 compatible object storage system. S3A is the most feature complete and robust client for interacting with S3 compatible object storage systems, successor to the S3N and S3 filesystem clients. While S3A makes every attempt to map the HDFS API closely to the S3 API, there are some semantic differences inherrent to object storage (Reference: Hadoop Wiki):\n\n\n\n\nEventual Consistency, with Amazon S3, or Multi-site Ceph. Creation, updates, deletes may not be visible for an undefined time.\n\n\nNon-atomic rename and delete operations. Renaming or deleting large directories takes time proportional to the number of entries and visible to other processes during this time, and indeed, until the eventual consistency has been resolved.\n\n\n\n\nEndpoints, credentials, and SSL\n#\n\n\nThe default endpoint for S3A routes requests to Amazon S3 with SSL (TLS). When interacting with a Ceph object store you will need to change a few S3A configuration parameters, notably the endpoint should be the DNS name or IP address of your Ceph object storage API endpoint.\n\n\n\n\nEndpoint (fs.s3a.endpoint)\n\n\nSSL (fs.s3a.connection.ssl.enabled)\n\n\n\n\n\n\nTip\n\n\nIf HTTP(S) is included in the endpoint definition, then the SSL property is automatically adjusted as appropriate. We will use this convention in the later module when we update the Jupyter Notebook.\n\n\n\n\nLoading the sample data set into Ceph object storage\n#\n\n\nTo load the data set into your Ceph object store we will use S3cmd, a python CLI tool for interacting with S3 compatible object stores. For convenience S3cmd was pre-installed on \nceph-node1\n.\n\n\n\n\n\n\nLogin to \nceph-node1\n as \nstudent\n user\n\n\n\n\n\n\nTo begin download the sample data set on \nceph-node\n from Amazon S3:\n\n\n\n\n\n\nwget -O /home/student/kubelet_docker_operations_latency_microseconds.zip https://s3.amazonaws.com/bd-dist/kubelet_docker_operations_latency_microseconds.zip\n\n\n\n\n\n\nUnzip the sample dataset\n\n\n\n\nunzip /home/student/kubelet_docker_operations_latency_microseconds.zip -d /home/student/METRICS\n\n\n\n\n\n\nTip\n\n\nThe S3 API provides two ways to route requests to a particular bucket. The first is to use a bucket domain name prefix, meaning the API request will be sent to whichever IP address the \n.\n hostname resolves to. If a wildcard DNS subdomain is not configured for the S3 endpoint, or if the endpoint domain name is not configured in Ceph, then requests using this convention will fail. The second way of routing requests is to use path style access, meaning the API request will be sent to \n/\n. We will create a bucket with all upper case letters, as this convention instructs the S3A client to use the latter, path style approach.\n\n\n\n\n\n\nCreate bucket for loading the sample data set into Ceph object store\n\n\n\n\ns3cmd mb s3://METRICS\n\n\n\n\nNext, we will use the \nsync\n S3cmd command to synchronize the local directory with the downloaded data set with the newly created bucket. This is roughly equivalent to using rsync between two filesystems.\n\n\n\n\nSync local directory with METRICS bucket\n\n\n\n\ns3cmd sync /home/student/auto-pilot/kubelet_docker_operations_latency_microseconds/ s3://METRICS\n\n\n\n\n\n\nNow we have our sample dataset ready to be used in Ceph object store.\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-3. In this module, you downloaded the sample data set, uploaded it to your Ceph object store using S3cmd. In the next module you will do some basic analysis on this dataa", 
            "title": "Module-3 : Introduction to S3A & Loading Sample Data Set"
        }, 
        {
            "location": "/Module-3/#module-3-introduction-to-s3a-loading-sample-data-set", 
            "text": "Module Agenda   In this module, you will be introduced to the S3A filesystem client  Download a sample data set of operational data.  Create a bucket and ingest the local data set to your Ceph object store     From your workstation, login to the  ceph-node1  node as user  student   (Learn how to Login)   ssh student@ IP Address of ceph-node1 node    Prerequisite   You need to have completed Modules 1-3 before beginning this module", 
            "title": "Module - 3 : Introduction to S3A &amp; Loading Sample Data Set"
        }, 
        {
            "location": "/Module-3/#introduction", 
            "text": "The S3A filesystem client comes from Apache Hadoop and can be utilized by Spark and other tools to interact with a S3 compatible object storage system. S3A is the most feature complete and robust client for interacting with S3 compatible object storage systems, successor to the S3N and S3 filesystem clients. While S3A makes every attempt to map the HDFS API closely to the S3 API, there are some semantic differences inherrent to object storage (Reference: Hadoop Wiki):   Eventual Consistency, with Amazon S3, or Multi-site Ceph. Creation, updates, deletes may not be visible for an undefined time.  Non-atomic rename and delete operations. Renaming or deleting large directories takes time proportional to the number of entries and visible to other processes during this time, and indeed, until the eventual consistency has been resolved.", 
            "title": "Introduction"
        }, 
        {
            "location": "/Module-3/#endpoints-credentials-and-ssl", 
            "text": "The default endpoint for S3A routes requests to Amazon S3 with SSL (TLS). When interacting with a Ceph object store you will need to change a few S3A configuration parameters, notably the endpoint should be the DNS name or IP address of your Ceph object storage API endpoint.   Endpoint (fs.s3a.endpoint)  SSL (fs.s3a.connection.ssl.enabled)    Tip  If HTTP(S) is included in the endpoint definition, then the SSL property is automatically adjusted as appropriate. We will use this convention in the later module when we update the Jupyter Notebook.", 
            "title": "Endpoints, credentials, and SSL"
        }, 
        {
            "location": "/Module-3/#loading-the-sample-data-set-into-ceph-object-storage", 
            "text": "To load the data set into your Ceph object store we will use S3cmd, a python CLI tool for interacting with S3 compatible object stores. For convenience S3cmd was pre-installed on  ceph-node1 .    Login to  ceph-node1  as  student  user    To begin download the sample data set on  ceph-node  from Amazon S3:    wget -O /home/student/kubelet_docker_operations_latency_microseconds.zip https://s3.amazonaws.com/bd-dist/kubelet_docker_operations_latency_microseconds.zip   Unzip the sample dataset   unzip /home/student/kubelet_docker_operations_latency_microseconds.zip -d /home/student/METRICS   Tip  The S3 API provides two ways to route requests to a particular bucket. The first is to use a bucket domain name prefix, meaning the API request will be sent to whichever IP address the  .  hostname resolves to. If a wildcard DNS subdomain is not configured for the S3 endpoint, or if the endpoint domain name is not configured in Ceph, then requests using this convention will fail. The second way of routing requests is to use path style access, meaning the API request will be sent to  / . We will create a bucket with all upper case letters, as this convention instructs the S3A client to use the latter, path style approach.    Create bucket for loading the sample data set into Ceph object store   s3cmd mb s3://METRICS  Next, we will use the  sync  S3cmd command to synchronize the local directory with the downloaded data set with the newly created bucket. This is roughly equivalent to using rsync between two filesystems.   Sync local directory with METRICS bucket   s3cmd sync /home/student/auto-pilot/kubelet_docker_operations_latency_microseconds/ s3://METRICS   Now we have our sample dataset ready to be used in Ceph object store.    End of Module  We have reached the end of Module-3. In this module, you downloaded the sample data set, uploaded it to your Ceph object store using S3cmd. In the next module you will do some basic analysis on this dataa", 
            "title": "Loading the sample data set into Ceph object storage"
        }, 
        {
            "location": "/Module-4/", 
            "text": "Module - 4 : Analytics on Metrics Data\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be doing basic analysis of Metrics Data Set\n\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou need to have completed Modules 1-3 before beginning this module\n\n\n\n\n\n\n\n\n\n\nThe instruction for this excercise is available as Juypter Notebook (\n.ipynb\n), that you could download it from \nhere\n\n\n\n\n\n\nYou would require Jupyter instance to open this notebook. Use JupyterHub application that you have deployed in module-2. Login to OpenShift Container Platform Console and click on JupyterHub application endpoint URL.\n\n\n\n\n\n\nUse the following credentials to login into the Jupyter Notebook\n\nUser Name : \nuser1\n\nPassword  : \n79e4e0\n  \n\n\n\n\n\n\nSelect \n\n\n\n\n\n\nClick \nStart Server\n\n\n\n\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-4. In this module, you downloaded the sample data set, uploaded it to your Ceph object store using S3cmd. In the next module you will do some basic analysis on this dataa", 
            "title": "Module-4 : Analytics on Metrics Data"
        }, 
        {
            "location": "/Module-4/#module-4-analytics-on-metrics-data", 
            "text": "Module Agenda   In this module, you will be doing basic analysis of Metrics Data Set     Prerequisite   You need to have completed Modules 1-3 before beginning this module      The instruction for this excercise is available as Juypter Notebook ( .ipynb ), that you could download it from  here    You would require Jupyter instance to open this notebook. Use JupyterHub application that you have deployed in module-2. Login to OpenShift Container Platform Console and click on JupyterHub application endpoint URL.    Use the following credentials to login into the Jupyter Notebook \nUser Name :  user1 \nPassword  :  79e4e0       Select     Click  Start Server      End of Module  We have reached the end of Module-4. In this module, you downloaded the sample data set, uploaded it to your Ceph object store using S3cmd. In the next module you will do some basic analysis on this dataa", 
            "title": "Module - 4 : Analytics on Metrics Data"
        }, 
        {
            "location": "/Module-5/", 
            "text": "Module - 5 : ML using Jupyter Datahub on Ceph\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be doing basic analysis of Metrics Data Set\n\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou need to have completed Modules 1-3 before beginning this module\n\n\n\n\n\n\n\n\n\n\nThe instruction for this excercise has been provided to you as a Jupyter Notebook. To access that, you need to open JupyterHub application endpoint, that you have deployed in module-2.\n\n\n\n\n\n\nUse the following credentials to login into the Jupyter Notebook\n\nUser Name : \nuser1\n\nPassword  : \n79e4e0\n  \n\n\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-4. In this module, you downloaded the sample data set, uploaded it to your Ceph object store using S3cmd. In the next module you will do some basic analysis on this dataa", 
            "title": "Module-5 : ML using Jupyter Datahub on Ceph"
        }, 
        {
            "location": "/Module-5/#module-5-ml-using-jupyter-datahub-on-ceph", 
            "text": "Module Agenda   In this module, you will be doing basic analysis of Metrics Data Set     Prerequisite   You need to have completed Modules 1-3 before beginning this module      The instruction for this excercise has been provided to you as a Jupyter Notebook. To access that, you need to open JupyterHub application endpoint, that you have deployed in module-2.    Use the following credentials to login into the Jupyter Notebook \nUser Name :  user1 \nPassword  :  79e4e0        End of Module  We have reached the end of Module-4. In this module, you downloaded the sample data set, uploaded it to your Ceph object store using S3cmd. In the next module you will do some basic analysis on this dataa", 
            "title": "Module - 5 : ML using Jupyter Datahub on Ceph"
        }
    ]
}